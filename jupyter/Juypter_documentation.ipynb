{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for Research Paper Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c166d0-2deb-41d0-b869-ab3c063d4fcf",
   "metadata": {},
   "source": [
    "### SE research paper chatbot\n",
    "\n",
    "Group Name: [csusb_fall2024_cse6550_team4](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)\n",
    "\n",
    "Instructor: Dr. Alzahrani, Nabeel\n",
    "\n",
    "Course: CSE 6550: Software Engineer Concepts Fall 2024\n",
    "\n",
    "Source: [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082af1f-a595-4647-b840-ea3b214a4e29",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851e3b-26b9-4cdb-8981-22e6b56c1625",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "The purpose of this project is to create an AI-powered research paper chatbot that helps users extract, summarize, and understand content from academic papers. It will offer an interactive Q&A experience, providing accurate, contextually relevant answers to questions about specific sections, making complex research information more accessible and comprehensible.\n",
    "\n",
    "Objective:\n",
    "\n",
    "The Paper Chatbot enhances interaction with academic papers by allowing users to upload documents, ask questions, and receive summaries or clarifications. It simplifies extracting key information, aiding students, researchers, and professionals in efficiently understanding complex content.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Jupyter Notebook, Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32172f-8f08-4c1a-8ed0-8713f13d28ea",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "The code sets up an environment for building a document processing and retrieval system that utilizes LangChain, MistralAI, and Milvus for vector storage. It aims to load, split, store, and analyze documents using machine learning models to support various NLP applications such as document retrieval or question-answering\n",
    "\n",
    "Input:\n",
    "- Environment variables loaded from a `.env` file (e.g., `MISTRAL_API_KEY`)\n",
    "- Documents from `CORPUS_SOURCE`, potentially PDFs from a directory (`PyPDFDirectoryLoader`), or web content via `WebBaseLoader` and `RecursiveUrlLoader`\n",
    "- Model information (`MODEL_NAME`) and storage directory paths (`data_dir`, `MILVUS_URI`)\n",
    "\n",
    "Output:\n",
    "- Status messages confirming imported libraries successfully\n",
    "- Prepared documents stored in Milvus as vector embeddings and configure tools for document retrieval, response generation chains  \n",
    "\n",
    "Processing:\n",
    "- Library imports: Loads libraries for document processing, NLP, and vector storage\n",
    "- Environment setup: Secures API keys using `dotenv`\n",
    "- Document preparation: Loads documents from sources (web/PDFs) and splits them using `RecursiveCharacterTextSplitter`\n",
    "- Model initialization: Uses `HuggingFaceEmbeddings` for text embeddings, with optional MistralAI/Cohere integration\n",
    "- Vector database: Connects to Milvus for storing and managing document embeddings using `pymilvus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported libraries successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "#from langchain_cohere import ChatCohere\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "from data import CORPUS_SOURCE\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from roman import toRoman\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "data_dir = \"./volumes\"\n",
    "print(\"imported libraries successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27531a9-789b-4cc1-8657-25884ab84b5c",
   "metadata": {},
   "source": [
    "### Configuring environment variables\n",
    "\n",
    "Purpose: The code initializes an environment for NLP applications, specifically for document retrieval and embedding generation using machine learning models.\n",
    "\n",
    "Input:\n",
    "\n",
    "- Environment variables loaded from a `.env` file (`MISTRAL_API_KEY` and `USER_AGENT`)\n",
    "- Configuration values such as `MILVUS_URI`, `MODEL_NAME`, and `CORPUS_SOURCE`\n",
    "\n",
    "Output:\n",
    "Confirms successful setup of the environment, document source readiness, and embedding model configuration for future processing.\n",
    "\n",
    "Processing:\n",
    "- Environment setup: Loads environment variables to set up API access and a user agent for HTTP requests.\n",
    "- Configuration: Prepares paths and settings for embedding models and vector storage.\n",
    "- Embedding preparation: Sets up a model (`sentence-transformers/all-MiniLM-L6-v2`) for processing documents and generating embeddings.\n",
    "- Document retrieval: References a document source URL (`CORPUS_SOURCE`) as a placeholder for further processing or document loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d114773-049b-41cb-b7fa-84da1b114c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")\n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT\n",
    "\n",
    "# Configuration settings  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  \n",
    "\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112fc9-ac1b-4d9f-9b3b-c6c915a642e4",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "Purpose: To create and return an embedding function configured with a specified NLP model for generating text embedding\n",
    "\n",
    "Input:\n",
    "- Model Name (`MODEL_NAME`): A pre-defined string representing the name of the model (e.g., `\"sentence-transformers/all-MiniLM-L6-v2\"`). This value is set globally and used to initialize the embedding function.\n",
    "\n",
    "Output:\n",
    "Returns an embedding function configured with the given model. This function can then be used to generate embeddings for text data.\n",
    "\n",
    "Processing:\n",
    "- The function creates an instance of `HuggingFaceEmbeddings` using `MODEL_NAME`, setting up an embedding generator based on the specified model.\n",
    "- It returns this configured embedding generator for use in NLP tasks like document analysis or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    return embedding_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Prompt Generator\n",
    "\n",
    "Purpose: To set up configurations for an NLP system and provide a placeholder function for generating prompts.\n",
    "\n",
    "Input:\n",
    "- Model Name: A string (`\"sentence-transformers/all-MiniLM-L6-v2\"`) representing the embedding model.\n",
    "- Milvus URI: A string (`\"./milvus/milvus_vector.db\"`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- A configured environment ready for NLP tasks.\n",
    "- `create_prompt()` function returns a string template to be used for generating detailed research summaries.\n",
    "\n",
    "Processing:\n",
    "- Configuration: Sets the model name for embedding generation and the URI for the vector database connection.\n",
    "- Prompt creation: Defines a simple placeholder function create_prompt() that returns a formatted prompt template for use in text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cc1b23-745d-4502-98e5-7a6c24a02e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "# Specifies the URI for the Milvus vector database\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ee2a-ceea-4610-b143-4ce770555072",
   "metadata": {},
   "source": [
    "### Vector store loader\n",
    "\n",
    "Purpose: To simulate the loading of a vector store from a specified URI and provide an interface for data retrieval.\n",
    "\n",
    "Input:\n",
    "URI: A string representing the location of the vector database (e.g., `MILVUS_URI`).\n",
    "\n",
    "Output:\n",
    "`VectorStore` object with an as_retriever method that simulates returning itself for further operations. This acts as a placeholder for actual vector store functionality.\n",
    "\n",
    "Processing:\n",
    "- The function defines a placeholder `VectorStore` class with an `as_retriever` method that returns the `VectorStore` instance itself, simulating a `vector store` capable of retrieval operations.\n",
    "- The function returns an instance of `VectorStore` initialized when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b709a8b-dab0-4ca4-b84b-76931094b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the vector store\n",
    "def load_exisiting_db(uri):\n",
    "    class VectorStore:  \n",
    "        def as_retriever(self): \n",
    "            return self \n",
    "    return VectorStore()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a789-ab91-44fb-8473-94745f96c5df",
   "metadata": {},
   "source": [
    "### Document Chain Placeholder\n",
    "\n",
    "Purpose: To serve as a placeholder function for creating a document processing chain using a model and a prompt.\n",
    "\n",
    "Input:\n",
    "- Model: An object representing an NLP model that will be used in the document chain.\n",
    "- Prompt: A string or object representing the prompt template to guide document processing.\n",
    "\n",
    "Output:\n",
    "\n",
    "Returns a string: \"Document chain here.\" to signify that a document processing chain has been set up (used for demonstration or placeholder purposes).\n",
    "\n",
    "Processing:\n",
    "- The function takes the model and prompt as arguments but currently does not perform any operations on them.\n",
    "- It returns a placeholder string indicating that a document chain has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c360e806-8f44-482d-9319-08436048cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    return \"Document chain here.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b112bbe-62d0-4e54-b368-ac54a05d3531",
   "metadata": {},
   "source": [
    "### Create Retrieval Chain\n",
    "\n",
    "Purpose: To act as a placeholder function for setting up a retrieval chain that uses a retriever and a document chain to produce a response with contextual information.\n",
    "\n",
    "Input:\n",
    "- Retriever: An object or method used to retrieve relevant data.\n",
    "- Document Chain: A chain that processes documents to generate responses or insights.\n",
    "\n",
    "Output:\n",
    "- Returns a lambda function that, when called, returns a dictionary with contextual metadata and a sample generated answer. \n",
    "- This simulates the behavior of a retrieval chain that would provide an answer based on retrieved data.\n",
    "\n",
    "Processing:\n",
    "- The function returns a lambda function that simulates generating a response. This lambda takes an input (`x`) and returns a dictionary containing:\n",
    "    - Context: List of dictionaries with metadata (e.g., source URLs) to simulate source references for the response.\n",
    "    - Answer: Placeholder string `\"Generated response\"` representing the result produced by the retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0b6231-bb1c-433f-a130-194a7c762568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a retrieval chain\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    return lambda x: {  # Returns a lambda function to generate a response\n",
    "        \"context\": [  # Contextual information with metadata for sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, \n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ], \n",
    "        \"answer\": \"Generated response\"  # Sample generated answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dd006-44be-4142-97a3-361340b2a5d0",
   "metadata": {},
   "source": [
    "# 4. Improving the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Improving Chatbot with RAG and Embeddings\n",
    "Purpose: To create and manage a vector store using Milvus, which stores embeddings generated from web documents for NLP applications.\n",
    "\n",
    "Input:\n",
    "- Documents: Text content loaded from the web through the `load_documents_from_web()` function.\n",
    "- Embedding Function: A function that converts document text into numerical embeddings.\n",
    "- URI: A file path (`'./milvus/milvus_vector.db'`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- Prints console messages at each step, confirming the successful execution of loading, processing, and vector store management.\n",
    "- If errors occur, an exception message is printed.\n",
    "\n",
    "Processing:\n",
    "- Document loading: Retrieves documents from a web source.\n",
    "- Document splitting: Splits the loaded documents into manageable chunks.\n",
    "- Embedding generation: Uses an embedding function to generate fixed-size numerical embeddings for each document chunk.\n",
    "- Vector store creation:\n",
    "   - Connects to Milvus.\n",
    "   - Checks if a collection (\"research_paper_chatbot\") exists. If not, creates a new collection schema.\n",
    "   - Inserts document embeddings into the collection.\n",
    "- Vector store loading: Loads an existing vector store collection from Milvus for retrieval operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Creating new Vector Store...\n",
      "New Vector Store Created with provided documents\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'\n",
    "\n",
    "# Placeholder function to simulate loading documents from the web\n",
    "def load_documents_from_web():\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]\n",
    "\n",
    "# Splits documents into smaller chunks\n",
    "def split_documents(documents):\n",
    "    return [doc for doc in documents]\n",
    "\n",
    "# Placeholder function to simulate getting an embedding function\n",
    "def get_embedding_function():\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Collection(name=collection_name)\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)\n",
    "            collection.insert([[embedding]])\n",
    "\n",
    "    return collection\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Loading documents from the web...\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "        print(\"Splitting documents into chunks...\")\n",
    "        docs = split_documents(documents)\n",
    "        print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "        print(\"Getting embedding function...\")\n",
    "        embeddings = get_embedding_function()\n",
    "\n",
    "        uri = './milvus/milvus_vector.db'\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)\n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00ea3b-e59b-43d2-a7d2-1ceebb58dffc",
   "metadata": {},
   "source": [
    "### Query Response Generator\n",
    "Purpose: To provide an entry point for querying a Retrieval-Augmented Generation (RAG) system, generate a response using NLP models, and include source references\n",
    "\n",
    "\n",
    "Input:\n",
    "Query: A string containing the user's question or request for information\n",
    "\n",
    "Output:\n",
    "\n",
    "Response: A string containing the generated answer with source references.\n",
    "Sources: A list of URLs or source identifiers referenced in the response.\n",
    "\n",
    "Processing:\n",
    "1. Model and Prompt Initialization:\n",
    "    - Loads a ChatMistralAI model and creates a prompt template with create_prompt() for structured input.\n",
    "2. Vector Store Retrieval:\n",
    "    - Loads an existing vector store from Milvus and creates a retriever to fetch relevant documents.\n",
    "3. Document Chain Creation:\n",
    "    - Uses `create_stuff_documents_chain()` to set up a document processing chain.\n",
    "4. Retrieval Chain Setup:\n",
    "    - Constructs a retrieval chain that integrates the retriever and document chain to process the query.\n",
    "5. Query Handling and Response Generation:\n",
    "    - Generates a response using the retrieval chain.\n",
    "    - Catches and handles HTTPStatusError for handling service load issues (e.g., error 429 for high traffic).\n",
    "6. Source Attribution:\n",
    "    - Extracts and formats up to four unique sources from the response context to include in the output.\n",
    "    - Appends source references to the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a prompt for the model.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: An instance of PromptTemplate for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"context\"],  \n",
    "        template=\"Given the following context: {context}, provide a comprehensive response.\"\n",
    "    )\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    model = ChatMistralAI(model='open-mistral-7b')  \n",
    "    print(\"Model Loaded\") \n",
    "\n",
    "    prompt = create_prompt()  \n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)  \n",
    "    retriever = vector_store.as_retriever()  \n",
    "    \n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)  \n",
    "        print(\"Document Chain Created\") \n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)  \n",
    "        print(\"Retrieval Chain Created\")  \n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:  \n",
    "        print(f\"HTTPStatusError: {e}\") \n",
    "        if e.response.status_code == 429:  \n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []  \n",
    "        return f\"HTTPStatusError: {e}\", []  \n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  \n",
    "    all_sources = \"\"  \n",
    "    sources = []  \n",
    "    count = 1  \n",
    "\n",
    "    for i in range(max_relevant_sources):  \n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"] \n",
    "            if source not in sources:  \n",
    "                sources.append(source)  \n",
    "                all_sources += f\"[Source {count}]({source}), \"  \n",
    "                count += 1  \n",
    "        except IndexError:  \n",
    "            break  \n",
    "            \n",
    "    all_sources = all_sources[:-2]  \n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"  \n",
    "    print(\"Response Generated\")  \n",
    "\n",
    "    return response[\"answer\"], sources  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f947a2-7d09-4715-882a-026e2de7f797",
   "metadata": {},
   "source": [
    " ### Adding Evaluation Metrics with Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775a587-3e61-4027-9819-e305f89dbac7",
   "metadata": {},
   "source": [
    "Purpose: To manage and store performance metrics related to a chatbot's classification performance (e.g., True Positives, False Positives, Accuracy, etc.).\n",
    "\n",
    "Inputs:\n",
    "- `increment_value (integer)`: Value to increment or update specific metrics.\n",
    "- `metric (string)`: The name of the metric to update.\n",
    "- `columns (string)`: The columns to fetch from the database.\n",
    "\n",
    "Processing:\n",
    "- Interacts with an database to execute queries.\n",
    "- Performs necessary calculations for metrics like sensitivity, specificity, precision, recall, F1 score. \n",
    "- Safely handles division to avoid division by zero errors.\n",
    "\n",
    "Outputs:\n",
    "Updates or retrieves performance metrics stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c88ec4d-3664-40a3-a2fc-d3b49bfa25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance_metrics(self):\n",
    "    \"\"\"\n",
    "    Recalculate and update the performance metrics in the database.\n",
    "    \"\"\"\n",
    "    metrics = self.get_performance_metrics('true_positive, true_negative, false_positive, false_negative')\n",
    "    accuracy = self.safe_division(metrics['true_positive'] + metrics['true_negative'], \n",
    "                                  metrics['true_positive'] + metrics['true_negative'] + metrics['false_positive'] + metrics['false_negative'])\n",
    "    precision = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_positive'])\n",
    "    sensitivity = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_negative'])\n",
    "    specificity = self.safe_division(metrics['true_negative'], metrics['true_negative'] + metrics['false_positive'])\n",
    "    recall = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_negative'])\n",
    "\n",
    "    if precision and sensitivity:\n",
    "        f1_score = self.safe_division(2 * precision * sensitivity, precision + sensitivity)\n",
    "    else:\n",
    "        f1_score = None\n",
    "\n",
    "    with self.connection:\n",
    "        self.connection.execute('''\n",
    "            UPDATE performance_metrics\n",
    "            SET accuracy = ?, precision = ?, sensitivity = ?, specificity = ?, f1_score = ?, recall = ?\n",
    "            WHERE id = 1\n",
    "        ''', (accuracy, precision, sensitivity, specificity, f1_score, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f3bc3-1322-4413-b911-001d448dbb45",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot\n",
    "\n",
    "Purpose: Test the chatbot with a sample query\n",
    "\n",
    "Input: Sample queries and documents. \n",
    "\n",
    "Output: Responses and any errors encountered. \n",
    "\n",
    "Processing: Test the RAG modelâ€™s response to ensure correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff82ec-d178-48b9-8593-5c8098548ca8",
   "metadata": {},
   "source": [
    "### Retrieve an answer from RAG.\n",
    "\n",
    "Submit a question to the RAG system, get the response, and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5ea4c-3d83-4a5a-8610-665db1feb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, sources = query_rag(\"How does LLm's work??\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5e524-4279-4566-a56f-9e8a77de00ea",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap: We set up a retrieval-augmented generation (RAG) chatbot using LangChain and Milvus, integrated PDF loading, and improved its functionality.\n",
    "\n",
    "Next Steps: Consider enhancing the chatbot with real-time data retrieval or using more advanced NLP techniques.\n",
    "\n",
    "Resources: For more details, visit [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4) and \n",
    "[Wiki for reference](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
