{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for Research Paper Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c166d0-2deb-41d0-b869-ab3c063d4fcf",
   "metadata": {},
   "source": [
    "### SE research paper chatbot\n",
    "\n",
    "Group Name: [csusb_fall2024_cse6550_team4](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)\n",
    "\n",
    "Instructor: Dr. Alzahrani, Nabeel\n",
    "\n",
    "Course: CSE 6550: Software Engineer Concepts Fall 2024\n",
    "\n",
    "Source: [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082af1f-a595-4647-b840-ea3b214a4e29",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851e3b-26b9-4cdb-8981-22e6b56c1625",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "The purpose of this project is to create an AI-powered research paper chatbot that helps users extract, summarize, and understand content from academic papers. It will offer an interactive Q&A experience, providing accurate, contextually relevant answers to questions about specific sections, making complex research information more accessible and comprehensible.\n",
    "\n",
    "Objective:\n",
    "\n",
    "The Paper Chatbot enhances interaction with academic papers by allowing users to upload documents, ask questions, and receive summaries or clarifications. It simplifies extracting key information, aiding students, researchers, and professionals in efficiently understanding complex content.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Jupyter Notebook, Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32172f-8f08-4c1a-8ed0-8713f13d28ea",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "Purpose: Load environment variables and setup libraries for the chatbot.\n",
    "\n",
    "Input: Necessary libraries and API key setup. \n",
    "\n",
    "Output: Environment variables and libraries loaded successfully \n",
    "\n",
    "Processing: Import necessary packages and initialize configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e4f62-5751-4bfa-996e-b549d39e1cb4",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "This Python script sets up an environment for processing and interacting with documents, particularly academic papers. It imports libraries for tasks like document loading, splitting, embedding generation, and integrates tools such as ChatMistralAI and HuggingFaceEmbeddings, along with a Milvus vector database for efficient data retrieval. Environment variables are loaded for secure API access, and paths are set for data storage and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported libraries successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "#from langchain_cohere import ChatCohere\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from roman import toRoman\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "data_dir = \"./volumes\"\n",
    "print(\"imported libraries successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27531a9-789b-4cc1-8657-25884ab84b5c",
   "metadata": {},
   "source": [
    "### Configuring environment variables\n",
    "\n",
    "Initialize the required environment variables for document processing. `CORPUS_SOURCE` can be updated to change the document source, `MISTRAL_API_KEY` holds the API key for MistralAI, `MILVUS_URI` defines the path to the Milvus Lite database file, and `MODEL_NAME` sets the embedding model used for analyzing the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d114773-049b-41cb-b7fa-84da1b114c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")\n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT\n",
    "\n",
    "# Configuration settings  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  \n",
    "\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112fc9-ac1b-4d9f-9b3b-c6c915a642e4",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd12927-ab6d-48e1-b8ae-9c5fe4f86f71",
   "metadata": {},
   "source": [
    "Purpose: Create the basic structure and functions for the chatbot logic.\n",
    "\n",
    "Input: User queries for processing.\n",
    "\n",
    "Output: Chatbot-generated responses. \n",
    "\n",
    "Processing: Set up the RAG model and vector store retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "\n",
    "- The `get_embedding_function` function then initializes the embedding function with the specified model and prints a confirmation message. \n",
    "\n",
    "- Calling `get_embedding_function` creates and stores this embedding function in `embedding_function`. Finally, it prints a message confirming that the embedding function was created successfully with the specified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    return embedding_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Prompt Generator\n",
    "\n",
    "- The `create_prompt` function serves as a simple template generator for formulating queries related to research topics. When called, it returns a predefined string that instructs the model to provide a detailed summary of the latest research on a specified subject, indicated by the placeholder `{input}`. \n",
    "\n",
    "- This allows the function to be flexible, enabling users to input various research topics and receive contextually relevant summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8cc1b23-745d-4502-98e5-7a6c24a02e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "# Specifies the URI for the Milvus vector database\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ee2a-ceea-4610-b143-4ce770555072",
   "metadata": {},
   "source": [
    "### Vector store loader\n",
    "\n",
    "- `load_exisiting_db` function is a placeholder that simulates the loading of a vector store from a specified URI.\n",
    "\n",
    "- It defines a local class, `VectorStore`, which includes a method called `as_retriever` that allows the instance of `VectorStore` to be treated as a retriever. When called, this function returns an instance of the `VectorStore` class, providing a structure for further integration with a retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b709a8b-dab0-4ca4-b84b-76931094b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the vector store\n",
    "def load_exisiting_db(uri):\n",
    "    class VectorStore:  \n",
    "        def as_retriever(self): \n",
    "            return self \n",
    "    return VectorStore()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a789-ab91-44fb-8473-94745f96c5df",
   "metadata": {},
   "source": [
    "### Document Chain Placeholder\n",
    "\n",
    "- The `create_stuff_documents_chain` function is a placeholder designed to establish a document processing chain that integrates a specified model and a prompt. \n",
    "- Currently, it returns a string that signifies the intended location for the actual document chain implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c360e806-8f44-482d-9319-08436048cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    return \"Document chain here.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b112bbe-62d0-4e54-b368-ac54a05d3531",
   "metadata": {},
   "source": [
    "### Create Retrieval Chain\n",
    "\n",
    "- This function defines a placeholder for creating a retrieval chain, which integrates a retriever and a document chain to generate a response.\n",
    "  \n",
    "- It returns a lambda function that simulates the retrieval process by providing contextual information and a sample generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0b6231-bb1c-433f-a130-194a7c762568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a retrieval chain\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    return lambda x: {  # Returns a lambda function to generate a response\n",
    "        \"context\": [  # Contextual information with metadata for sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, \n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ], \n",
    "        \"answer\": \"Generated response\"  # Sample generated answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dd006-44be-4142-97a3-361340b2a5d0",
   "metadata": {},
   "source": [
    "# 4. Improving the Chatbot\n",
    "\n",
    "Purpose: Enhance the chatbot with NLP capabilities and optimize document retrieval. \n",
    "\n",
    "Input: Documents to split and user queries. \n",
    "\n",
    "Output: Improved and factually accurate responses. \n",
    "\n",
    "Processing: Use Milvus for vector storage and NLP for document processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Improving Chatbot with RAG and Embeddings\n",
    "\n",
    "The code sets up a retrieval-augmented generation (RAG) system using a Milvus vector store to manage document embeddings. It includes functions to load documents, create or load a vector store, and manage embedding schemas for storage. Progress is logged with print statements to inform the user of each operation's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Creating new Vector Store...\n",
      "New Vector Store Created with provided documents\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'\n",
    "\n",
    "# Placeholder function to simulate loading documents from the web\n",
    "def load_documents_from_web():\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]\n",
    "\n",
    "# Splits documents into smaller chunks\n",
    "def split_documents(documents):\n",
    "    return [doc for doc in documents]\n",
    "\n",
    "# Placeholder function to simulate getting an embedding function\n",
    "def get_embedding_function():\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Collection(name=collection_name)\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)\n",
    "            collection.insert([[embedding]])\n",
    "\n",
    "    return collection\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Loading documents from the web...\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "        print(\"Splitting documents into chunks...\")\n",
    "        docs = split_documents(documents)\n",
    "        print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "        print(\"Getting embedding function...\")\n",
    "        embeddings = get_embedding_function()\n",
    "\n",
    "        uri = './milvus/milvus_vector.db'\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)\n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00ea3b-e59b-43d2-a7d2-1ceebb58dffc",
   "metadata": {},
   "source": [
    "### Query Response Generator\n",
    "- The `query_rag` function serves as the main entry point for generating responses using a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "- It initializes the model and sets up the necessary components, including the prompt and retrieval mechanisms, to process a user query. After generating a response, it compiles a list of relevant sources used in the response, ensuring that users receive both the answer and the origins of the information provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a prompt for the model.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: An instance of PromptTemplate for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"context\"],  \n",
    "        template=\"Given the following context: {context}, provide a comprehensive response.\"\n",
    "    )\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    model = ChatMistralAI(model='open-mistral-7b')  \n",
    "    print(\"Model Loaded\") \n",
    "\n",
    "    prompt = create_prompt()  \n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)  \n",
    "    retriever = vector_store.as_retriever()  \n",
    "    \n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)  \n",
    "        print(\"Document Chain Created\") \n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)  \n",
    "        print(\"Retrieval Chain Created\")  \n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:  \n",
    "        print(f\"HTTPStatusError: {e}\") \n",
    "        if e.response.status_code == 429:  \n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []  \n",
    "        return f\"HTTPStatusError: {e}\", []  \n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  \n",
    "    all_sources = \"\"  \n",
    "    sources = []  \n",
    "    count = 1  \n",
    "\n",
    "    for i in range(max_relevant_sources):  \n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"] \n",
    "            if source not in sources:  \n",
    "                sources.append(source)  \n",
    "                all_sources += f\"[Source {count}]({source}), \"  \n",
    "                count += 1  \n",
    "        except IndexError:  \n",
    "            break  \n",
    "            \n",
    "    all_sources = all_sources[:-2]  \n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"  \n",
    "    print(\"Response Generated\")  \n",
    "\n",
    "    return response[\"answer\"], sources  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f947a2-7d09-4715-882a-026e2de7f797",
   "metadata": {},
   "source": [
    " ### Adding Evaluation Metrics with Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775a587-3e61-4027-9819-e305f89dbac7",
   "metadata": {},
   "source": [
    "Purpose: To manage and store performance metrics related to a chatbot's classification performance (e.g., True Positives, False Positives, Accuracy, etc.).\n",
    "\n",
    "Inputs:\n",
    "- `increment_value (integer)`: Value to increment or update specific metrics.\n",
    "- `metric (string)`: The name of the metric to update.\n",
    "- `columns (string)`: The columns to fetch from the database.\n",
    "\n",
    "Processing:\n",
    "- Interacts with an database to execute queries.\n",
    "- Performs necessary calculations for metrics like sensitivity, specificity, precision, recall, F1 score. \n",
    "- Safely handles division to avoid division by zero errors.\n",
    "\n",
    "Outputs:\n",
    "Updates or retrieves performance metrics stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c88ec4d-3664-40a3-a2fc-d3b49bfa25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance_metrics(self):\n",
    "    \"\"\"\n",
    "    Recalculate and update the performance metrics in the database.\n",
    "    \"\"\"\n",
    "    metrics = self.get_performance_metrics('true_positive, true_negative, false_positive, false_negative')\n",
    "    accuracy = self.safe_division(metrics['true_positive'] + metrics['true_negative'], \n",
    "                                  metrics['true_positive'] + metrics['true_negative'] + metrics['false_positive'] + metrics['false_negative'])\n",
    "    precision = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_positive'])\n",
    "    sensitivity = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_negative'])\n",
    "    specificity = self.safe_division(metrics['true_negative'], metrics['true_negative'] + metrics['false_positive'])\n",
    "    recall = self.safe_division(metrics['true_positive'], metrics['true_positive'] + metrics['false_negative'])\n",
    "\n",
    "    if precision and sensitivity:\n",
    "        f1_score = self.safe_division(2 * precision * sensitivity, precision + sensitivity)\n",
    "    else:\n",
    "        f1_score = None\n",
    "\n",
    "    with self.connection:\n",
    "        self.connection.execute('''\n",
    "            UPDATE performance_metrics\n",
    "            SET accuracy = ?, precision = ?, sensitivity = ?, specificity = ?, f1_score = ?, recall = ?\n",
    "            WHERE id = 1\n",
    "        ''', (accuracy, precision, sensitivity, specificity, f1_score, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f3bc3-1322-4413-b911-001d448dbb45",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot\n",
    "\n",
    "Purpose: Test the chatbot with a sample query\n",
    "\n",
    "Input: Sample queries and documents. \n",
    "\n",
    "Output: Responses and any errors encountered. \n",
    "\n",
    "Processing: Test the RAG modelâ€™s response to ensure correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff82ec-d178-48b9-8593-5c8098548ca8",
   "metadata": {},
   "source": [
    "### Retrieve an answer from RAG.\n",
    "\n",
    "Submit a question to the RAG system, get the response, and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5ea4c-3d83-4a5a-8610-665db1feb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, sources = query_rag(\"How does LLm's work??\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5e524-4279-4566-a56f-9e8a77de00ea",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap: We set up a retrieval-augmented generation (RAG) chatbot using LangChain and Milvus, integrated PDF loading, and improved its functionality.\n",
    "\n",
    "Next Steps: Consider enhancing the chatbot with real-time data retrieval or using more advanced NLP techniques.\n",
    "\n",
    "Resources: For more details, visit [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4) and \n",
    "[Wiki for reference](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
