{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for Research Paper Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c166d0-2deb-41d0-b869-ab3c063d4fcf",
   "metadata": {},
   "source": [
    "### SE research paper chatbot\n",
    "\n",
    "Group Name: [csusb_fall2024_cse6550_team4](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)\n",
    "\n",
    "Instructor: Dr. Alzahrani, Nabeel\n",
    "\n",
    "Course: CSE 6550: Software Engineer Concepts Fall 2024\n",
    "\n",
    "Source: [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082af1f-a595-4647-b840-ea3b214a4e29",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851e3b-26b9-4cdb-8981-22e6b56c1625",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "The purpose of this project is to create an AI-powered research paper chatbot that helps users extract, summarize, and understand content from academic papers. It will offer an interactive Q&A experience, providing accurate, contextually relevant answers to questions about specific sections, making complex research information more accessible and comprehensible.\n",
    "\n",
    "Objective:\n",
    "\n",
    "The Paper Chatbot enhances interaction with academic papers by allowing users to upload documents, ask questions, and receive summaries or clarifications. It simplifies extracting key information, aiding students, researchers, and professionals in efficiently understanding complex content.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Jupyter Notebook, Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32172f-8f08-4c1a-8ed0-8713f13d28ea",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "The code sets up an environment for building a document processing and retrieval system that utilizes LangChain, MistralAI, and Milvus for vector storage. It aims to load, split, store, and analyze documents using machine learning models to support various NLP applications such as document retrieval or question-answering\n",
    "\n",
    "Input:\n",
    "- Environment variables loaded from a `.env` file (e.g., `MISTRAL_API_KEY`)\n",
    "- Documents from `CORPUS_SOURCE`, potentially PDFs from a directory (`PyPDFDirectoryLoader`), or web content via `WebBaseLoader` and `RecursiveUrlLoader`\n",
    "- Model information (`MODEL_NAME`) and storage directory paths (`data_dir`, `MILVUS_URI`)\n",
    "\n",
    "Output:\n",
    "- Status messages confirming imported libraries successfully\n",
    "- Prepared documents stored in Milvus as vector embeddings and configure tools for document retrieval, response generation chains  \n",
    "\n",
    "Processing:\n",
    "- Library imports: Loads libraries for document processing, NLP, and vector storage\n",
    "- Environment setup: Secures API keys using `dotenv`\n",
    "- Document preparation: Loads documents from sources (web/PDFs) and splits them using `RecursiveCharacterTextSplitter`\n",
    "- Model initialization: Uses `HuggingFaceEmbeddings` for text embeddings, with optional MistralAI/Cohere integration\n",
    "- Vector database: Connects to Milvus for storing and managing document embeddings using `pymilvus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and USER_AGENT set successfully.\n",
      "USER_AGENT: CustomUserAgent\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the USER_AGENT environment variable\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"CustomUserAgent\")  # Default to \"CustomUserAgent\" if not set\n",
    "os.environ['USER_AGENT'] = USER_AGENT  \n",
    "\n",
    "# Confirmation messages for successful imports and setup\n",
    "print(\"Libraries imported and USER_AGENT set successfully.\")  # Inform that setup was successful\n",
    "print(f\"USER_AGENT: {USER_AGENT}\")  # Output the current USER_AGENT value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca9505-f31b-4ecd-8d3a-226f815b63d3",
   "metadata": {},
   "source": [
    "### Initialization and Configuration of Constants\n",
    "\n",
    "Purpose:\n",
    "To initialize the system by importing necessary libraries, validating critical environment variables, and configuring constants required for NLP tasks, vector database operations, and embedding generation.\n",
    "\n",
    "Input:\n",
    "1. Environment variables:\n",
    "`MISTRAL_API_KEY` (retrieved using `os.getenv()`).\n",
    "2. Constants:\n",
    "`MILVUS_URI`: Path to the Milvus vector database.\n",
    "`MODEL_NAME`: Name of the embedding model.\n",
    "\n",
    "Output:\n",
    "- Prints confirmation messages:\n",
    "    - Successful import of libraries and setup of constants.\n",
    "    - Values of `MILVUS_URI` and `MODEL_NAME`.\n",
    "- Raises an error if `MISTRAL_API_KEY` is not set:\n",
    "- `ValueError: \"MISTRAL_API_KEY environment variable not set.\"`\n",
    "\n",
    "Processing:\n",
    "- Checks for the `MISTRAL_API_KEY` in the environment:\n",
    "     - If missing, raises a ValueError.\n",
    "- Sets up constants (`MILVUS_URI` and `MODEL_NAME`) for subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c61470-c1c1-4bd6-83f5-65f7da69bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional libraries imported, and constants set successfully.\n",
      "MILVUS_URI: ./milvus/milvus_vector.db, MODEL_NAME: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "# from data import CORPUS_SOURCE\n",
    "CORPUS_SOURCE = None  # Placeholder\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from roman import toRoman\n",
    "\n",
    "# Validate MISTRAL_API_KEY and set constants\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(\"Additional libraries imported, and constants set successfully.\")\n",
    "print(f\"MILVUS_URI: {MILVUS_URI}, MODEL_NAME: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27531a9-789b-4cc1-8657-25884ab84b5c",
   "metadata": {},
   "source": [
    "### Configuring environment variables\n",
    "\n",
    "Purpose: The code initializes an environment for NLP applications, specifically for document retrieval and embedding generation using machine learning models.\n",
    "\n",
    "Input:\n",
    "\n",
    "- Environment variables loaded from a `.env` file (`MISTRAL_API_KEY` and `USER_AGENT`)\n",
    "- Configuration values such as `MILVUS_URI`, `MODEL_NAME`, and `CORPUS_SOURCE`\n",
    "\n",
    "Output:\n",
    "Confirms successful setup of the environment, document source readiness, and embedding model configuration for future processing.\n",
    "\n",
    "Processing:\n",
    "- Environment setup: Loads environment variables to set up API access and a user agent for HTTP requests.\n",
    "- Configuration: Prepares paths and settings for embedding models and vector storage.\n",
    "- Embedding preparation: Sets up a model (`sentence-transformers/all-MiniLM-L6-v2`) for processing documents and generating embeddings.\n",
    "- Document retrieval: References a document source URL (`CORPUS_SOURCE`) as a placeholder for further processing or document loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d114773-049b-41cb-b7fa-84da1b114c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()  # This loads environment variables defined in a .env file into the program\n",
    "\n",
    "# Retrieve the Mistral API key from the environment for authentication\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")  \n",
    "# Set the USER_AGENT variable\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")  \n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT \n",
    "\n",
    "# Configuration settings for Milvus database, model name, and corpus source\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  \n",
    "\n",
    "# Print confirmation of setup\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")  # Confirm successful setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112fc9-ac1b-4d9f-9b3b-c6c915a642e4",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "Purpose: To create and return an embedding function configured with a specified NLP model for generating text embedding\n",
    "\n",
    "Input:\n",
    "- Model Name (`MODEL_NAME`): A pre-defined string representing the name of the model (e.g., `\"sentence-transformers/all-MiniLM-L6-v2\"`). This value is set globally and used to initialize the embedding function.\n",
    "\n",
    "Output:\n",
    "Returns an embedding function configured with the given model. This function can then be used to generate embeddings for text data.\n",
    "\n",
    "Processing:\n",
    "- The function creates an instance of `HuggingFaceEmbeddings` using `MODEL_NAME`, setting up an embedding generator based on the specified model.\n",
    "- It returns this configured embedding generator for use in NLP tasks like document analysis or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0505601a409e42ddbdf1ff428fa71898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a336254e5dc34f4696c967323c7c9c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4e8d8601004b9bba0354515e93ef56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20d3593372e46b08c4ddb7546c2c5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a2d8453ee845ec8beee8933959f2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef56514f2a964ae1a6df43fb4b645cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c0a859cf474cf391cef4a53c44b76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99653be8420548b2b34ab1a87857f64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefcfb3645fc4ec09a4094a5b9c0bbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7484da6ce7cf4999841b55be403c5cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa79d3f069dd490aba899b48cbe4ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function created using model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding function initialized: client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#  Specify the HuggingFace model for embeddings\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns the embedding function for the specified model.\n",
    "\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings: An embedding function object.\n",
    "    \"\"\"\n",
    "    # Initialize the HuggingFace embedding function with the specified model name\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    # Print a message confirming the embedding function has been created\n",
    "    print(f\"Embedding function created using model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Return the initialized embedding function\n",
    "    return embedding_function\n",
    "\n",
    "# Call the embeddin function and display the output\n",
    "embedding_fn = get_embedding_function()  \n",
    "print(f\"Embedding function initialized: {embedding_fn}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Prompt Generator\n",
    "\n",
    "Purpose: To set up configurations for an NLP system and provide a placeholder function for generating prompts.\n",
    "\n",
    "Input:\n",
    "- Model Name: A string (`\"sentence-transformers/all-MiniLM-L6-v2\"`) representing the embedding model.\n",
    "- Milvus URI: A string (`\"./milvus/milvus_vector.db\"`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- A configured environment ready for NLP tasks.\n",
    "- `create_prompt()` function returns a string template to be used for generating detailed research summaries.\n",
    "\n",
    "Processing:\n",
    "- Configuration: Sets the model name for embedding generation and the URI for the vector database connection.\n",
    "- Prompt creation: Defines a simple placeholder function create_prompt() that returns a formatted prompt template for use in text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cc1b23-745d-4502-98e5-7a6c24a02e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: sentence-transformers/all-MiniLM-L6-v2\n",
      "MILVUS_URI: ./milvus/milvus_vector.db\n",
      "Prompt created: Provide a detailed summary of the latest research on: {input}\n",
      "Returned prompt: Provide a detailed summary of the latest research on: {input}\n"
     ]
    }
   ],
   "source": [
    "# Specifies the HuggingFace model for embedding generation and specifies the URI for the Milvus vector database\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Returns a placeholder prompt template.\n",
    "\n",
    "    Returns:\n",
    "        str: The prompt template.\n",
    "    \"\"\"\n",
    "    # Define a placeholder prompt for generating research summaries\n",
    "    prompt = \"Provide a detailed summary of the latest research on: {input}\"\n",
    "    print(f\"Prompt created: {prompt}\")  # Print confirmation of the created prompt\n",
    "    return prompt  # Return the created prompt template\n",
    "\n",
    "# Display the constants and call the function\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")  # Print the model name for reference\n",
    "print(f\"MILVUS_URI: {MILVUS_URI}\")  # Print the Milvus URI for reference\n",
    "\n",
    "prompt_template = create_prompt()  # Call the function to create a prompt template\n",
    "print(f\"Returned prompt: {prompt_template}\")  # Print the returned prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ee2a-ceea-4610-b143-4ce770555072",
   "metadata": {},
   "source": [
    "### Vector store loader\n",
    "\n",
    "Purpose: To simulate the loading of a vector store from a specified URI and provide an interface for data retrieval.\n",
    "\n",
    "Input:\n",
    "URI: A string representing the location of the vector database (e.g., `MILVUS_URI`).\n",
    "\n",
    "Output:\n",
    "`VectorStore` object with an as_retriever method that simulates returning itself for further operations. This acts as a placeholder for actual vector store functionality.\n",
    "\n",
    "Processing:\n",
    "- The function defines a placeholder `VectorStore` class with an `as_retriever` method that returns the `VectorStore` instance itself, simulating a `vector store` capable of retrieval operations.\n",
    "- The function returns an instance of `VectorStore` initialized when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b709a8b-dab0-4ca4-b84b-76931094b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from URI: ./milvus/milvus_vector.db\n",
      "Vector store loaded: <__main__.load_existing_db.<locals>.VectorStore object at 0xffff40337610>\n"
     ]
    }
   ],
   "source": [
    "# Function for loading the vector store\n",
    "def load_existing_db(uri):\n",
    "    \"\"\"\n",
    "    Simulates loading an existing vector store from a given URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str): The URI of the vector store database.\n",
    "\n",
    "    Returns:\n",
    "        VectorStore: A mock vector store class instance.\n",
    "    \"\"\"\n",
    "    # Define a mock VectorStore class with an as_retriever method\n",
    "    class VectorStore:\n",
    "        def as_retriever(self):\n",
    "            # Simulate returning a retriever instance\n",
    "            return self\n",
    "\n",
    "    print(f\"Loading vector store from URI: {uri}\")  # Print confirmation of the URI being used\n",
    "    return VectorStore()  # Return an instance of the mock VectorStore class\n",
    "\n",
    "# Specify the URI for the vector store\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # Path to the Milvus vector database\n",
    "\n",
    "# Call the function to load the vector store\n",
    "vector_store = load_existing_db(MILVUS_URI)\n",
    "print(f\"Vector store loaded: {vector_store}\")  # Print confirmation that the vector store was loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a789-ab91-44fb-8473-94745f96c5df",
   "metadata": {},
   "source": [
    "### Document Chain Placeholder\n",
    "\n",
    "Purpose: To serve as a placeholder function for creating a document processing chain using a model and a prompt.\n",
    "\n",
    "Input:\n",
    "- Model: An object representing an NLP model that will be used in the document chain.\n",
    "- Prompt: A string or object representing the prompt template to guide document processing.\n",
    "\n",
    "Output:\n",
    "\n",
    "Returns a string: \"Document chain here.\" to signify that a document processing chain has been set up (used for demonstration or placeholder purposes).\n",
    "\n",
    "Processing:\n",
    "- The function takes the model and prompt as arguments but currently does not perform any operations on them.\n",
    "- It returns a placeholder string indicating that a document chain has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c360e806-8f44-482d-9319-08436048cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document chain with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Using prompt: Provide a detailed summary of the latest research on: {input}\n",
      "Returned document chain: Document chain here.\n"
     ]
    }
   ],
   "source": [
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    \"\"\"\n",
    "    Simulates the creation of a document chain.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model to use.\n",
    "        prompt (str): The prompt template.\n",
    "\n",
    "    Returns:\n",
    "        str: A placeholder message indicating the document chain creation.\n",
    "    \"\"\"\n",
    "    # Print the model being used for creating the document chain\n",
    "    print(f\"Creating document chain with model: {model}\")\n",
    "    \n",
    "    # Print the prompt being used for the document chain\n",
    "    print(f\"Using prompt: {prompt}\")\n",
    "    \n",
    "    # Return a placeholder message for the document chain\n",
    "    return \"Document chain here.\"\n",
    "\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "\n",
    "# Define a test prompt for the document chain\n",
    "test_prompt = \"Provide a detailed summary of the latest research on: {input}\"  \n",
    "\n",
    "# Call the function to simulate creating a document chain\n",
    "document_chain = create_stuff_documents_chain(MODEL_NAME, test_prompt)\n",
    "\n",
    "# Print the result of the document chain creation\n",
    "print(f\"Returned document chain: {document_chain}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dd006-44be-4142-97a3-361340b2a5d0",
   "metadata": {},
   "source": [
    "# 4. Improving the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Improving Chatbot with RAG and Embeddings\n",
    "Purpose: To create and manage a vector store using Milvus, which stores embeddings generated from web documents for NLP applications.\n",
    "\n",
    "Input:\n",
    "- Documents: Text content loaded from the web through the `load_documents_from_web()` function.\n",
    "- Embedding Function: A function that converts document text into numerical embeddings.\n",
    "- URI: A file path (`'./milvus/milvus_vector.db'`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- Prints console messages at each step, confirming the successful execution of loading, processing, and vector store management.\n",
    "- If errors occur, an exception message is printed.\n",
    "\n",
    "Processing:\n",
    "- Document loading: Retrieves documents from a web source.\n",
    "- Document splitting: Splits the loaded documents into manageable chunks.\n",
    "- Embedding generation: Uses an embedding function to generate fixed-size numerical embeddings for each document chunk.\n",
    "- Vector store creation:\n",
    "   - Connects to Milvus.\n",
    "   - Checks if a collection (\"research_paper_chatbot\") exists. If not, creates a new collection schema.\n",
    "   - Inserts document embeddings into the collection.\n",
    "- Vector store loading: Loads an existing vector store collection from Milvus for retrieval operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Creating new Vector Store...\n",
      "New Vector Store Created with provided documents\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'\n",
    "\n",
    "# Placeholder function to simulate loading documents from the web\n",
    "def load_documents_from_web():\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]\n",
    "\n",
    "# Splits documents into smaller chunks\n",
    "def split_documents(documents):\n",
    "    return [doc for doc in documents]\n",
    "\n",
    "# Placeholder function to simulate getting an embedding function\n",
    "def get_embedding_function():\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Collection(name=collection_name)\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)\n",
    "            collection.insert([[embedding]])\n",
    "\n",
    "    return collection\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Loading documents from the web...\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "        print(\"Splitting documents into chunks...\")\n",
    "        docs = split_documents(documents)\n",
    "        print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "        print(\"Getting embedding function...\")\n",
    "        embeddings = get_embedding_function()\n",
    "\n",
    "        uri = './milvus/milvus_vector.db'\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)\n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00ea3b-e59b-43d2-a7d2-1ceebb58dffc",
   "metadata": {},
   "source": [
    "### Query Response Generator\n",
    "Purpose: To provide an entry point for querying a Retrieval-Augmented Generation (RAG) system, generate a response using NLP models, and include source references\n",
    "\n",
    "\n",
    "Input:\n",
    "Query: A string containing the user's question or request for information\n",
    "\n",
    "Output:\n",
    "\n",
    "Response: A string containing the generated answer with source references.\n",
    "Sources: A list of URLs or source identifiers referenced in the response.\n",
    "\n",
    "Processing:\n",
    "1. Model and Prompt Initialization:\n",
    "    - Loads a ChatMistralAI model and creates a prompt template with create_prompt() for structured input.\n",
    "2. Vector Store Retrieval:\n",
    "    - Loads an existing vector store from Milvus and creates a retriever to fetch relevant documents.\n",
    "3. Document Chain Creation:\n",
    "    - Uses `create_stuff_documents_chain()` to set up a document processing chain.\n",
    "4. Retrieval Chain Setup:\n",
    "    - Constructs a retrieval chain that integrates the retriever and document chain to process the query.\n",
    "5. Query Handling and Response Generation:\n",
    "    - Generates a response using the retrieval chain.\n",
    "    - Catches and handles HTTPStatusError for handling service load issues (e.g., error 429 for high traffic).\n",
    "6. Source Attribution:\n",
    "    - Extracts and formats up to four unique sources from the response context to include in the output.\n",
    "    - Appends source references to the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Created Successfully!\n",
      "Prompt Template:\n",
      "Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
      "    Only use the information provided in the <context> tags.\n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    {context}\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    {input}\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    Assistant:\n",
      "Returned PromptTemplate: input_variables=['context', 'input'] template=\"\\n    Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\\n    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\\n    Only use the information provided in the <context> tags.\\n    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n    <context>\\n    {context}\\n    </context>\\n\\n    <question>\\n    {input}\\n    </question>\\n\\n    The response should be specific and use statistics or numbers when possible.\\n\\n    Assistant:\"\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a prompt template for the AI assistant.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The created prompt template.\n",
    "    \"\"\"\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "    Only use the information provided in the <context> tags.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {input}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    # Create a PromptTemplate instance\n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    print(\"Prompt Created Successfully!\")\n",
    "    print(f\"Prompt Template:\\n{PROMPT_TEMPLATE.strip()}\")\n",
    "    return prompt\n",
    "\n",
    "# Call the function and print the returned value\n",
    "created_prompt = create_prompt()\n",
    "print(f\"Returned PromptTemplate: {created_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f947a2-7d09-4715-882a-026e2de7f797",
   "metadata": {},
   "source": [
    " ### Adding Evaluation Metrics with Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775a587-3e61-4027-9819-e305f89dbac7",
   "metadata": {},
   "source": [
    "Purpose: To manage and store performance metrics related to a chatbot's classification performance (e.g., True Positives, False Positives, Accuracy, etc.).\n",
    "\n",
    "Inputs:\n",
    "- `increment_value (integer)`: Value to increment or update specific metrics.\n",
    "- `metric (string)`: The name of the metric to update.\n",
    "- `columns (string)`: The columns to fetch from the database.\n",
    "\n",
    "Processing:\n",
    "- Interacts with an database to execute queries.\n",
    "- Performs necessary calculations for metrics like sensitivity, specificity, precision, recall, F1 score. \n",
    "- Safely handles division to avoid division by zero errors.\n",
    "\n",
    "Outputs:\n",
    "Updates or retrieves performance metrics stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c88ec4d-3664-40a3-a2fc-d3b49bfa25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance_metrics(self):\n",
    "    \"\"\"\n",
    "    Recalculate and update the performance metrics in the database.\n",
    "    \"\"\"\n",
    "    # Fetch metrics from the database\n",
    "    print(\"Fetching performance metrics from the database...\")\n",
    "    metrics = self.get_performance_metrics('true_positive, true_negative, false_positive, false_negative')\n",
    "    print(f\"Metrics retrieved: {metrics}\")  # Print the metrics retrieved for debugging\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = self.safe_division(\n",
    "        metrics['true_positive'] + metrics['true_negative'], \n",
    "        metrics['true_positive'] + metrics['true_negative'] + metrics['false_positive'] + metrics['false_negative']\n",
    "    )\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_positive']\n",
    "    )\n",
    "    \n",
    "    # Calculate sensitivity (also known as recall)\n",
    "    sensitivity = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_negative']\n",
    "    )\n",
    "    \n",
    "    # Calculate specificity\n",
    "    specificity = self.safe_division(\n",
    "        metrics['true_negative'], \n",
    "        metrics['true_negative'] + metrics['false_positive']\n",
    "    )\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_negative']\n",
    "    )\n",
    "\n",
    "    # Print calculated metrics for debugging\n",
    "    print(f\"Calculated Metrics:\\n\"\n",
    "          f\"Accuracy: {accuracy}\\n\"\n",
    "          f\"Precision: {precision}\\n\"\n",
    "          f\"Sensitivity: {sensitivity}\\n\"\n",
    "          f\"Specificity: {specificity}\\n\"\n",
    "          f\"Recall: {recall}\")\n",
    "\n",
    "    # Calculate F1 score if precision and sensitivity are available\n",
    "    if precision and sensitivity:\n",
    "        f1_score = self.safe_division(2 * precision * sensitivity, precision + sensitivity)\n",
    "    else:\n",
    "        f1_score = None  # Handle cases where F1 score cannot be calculated\n",
    "\n",
    "    print(f\"F1 Score: {f1_score}\")  # Print the F1 score for debugging\n",
    "\n",
    "    # Update the metrics in the database\n",
    "    print(\"Updating performance metrics in the database...\")\n",
    "    with self.connection:\n",
    "        self.connection.execute('''\n",
    "            UPDATE performance_metrics\n",
    "            SET accuracy = ?, precision = ?, sensitivity = ?, specificity = ?, f1_score = ?, recall = ?\n",
    "            WHERE id = 1\n",
    "        ''', (accuracy, precision, sensitivity, specificity, f1_score, recall))\n",
    "    print(\"Performance metrics updated successfully.\")  # Confirm the update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f3bc3-1322-4413-b911-001d448dbb45",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot\n",
    "\n",
    "Purpose: Test the chatbot with a sample query\n",
    "\n",
    "Input: Sample queries and documents. \n",
    "\n",
    "Output: Responses and any errors encountered. \n",
    "\n",
    "Processing: Test the RAG model’s response to ensure correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff82ec-d178-48b9-8593-5c8098548ca8",
   "metadata": {},
   "source": [
    "### Retrieve an answer from RAG.\n",
    "\n",
    "Submit a question to the RAG system, get the response, and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67df23e-967d-4212-946e-865f7ffc4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dff91a267454e8b8353a3da885e392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='80%'), placeholder='Enter your query here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca55c1550f443dc83f42dbf95549357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle(), tooltip='Click to submit your query'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd1c3d22f5949d09fc81829c41567f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a text input widget for the user to enter a prompt\n",
    "prompt_input = widgets.Text(\n",
    "    value='', \n",
    "    placeholder='Enter your query here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create a button to submit the prompt\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to submit your query'\n",
    ")\n",
    "\n",
    "# Create an output widget to display the response\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to handle button click and display the response\n",
    "def on_submit_click(change):\n",
    "    query = prompt_input.value\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "        print(f\"Query: {query}\")\n",
    "        # Simulate querying RAG and displaying the response\n",
    "        response, sources = query_rag(query)  # Replace with actual function call\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response)\n",
    "        print(\"\\nSources:\")\n",
    "        for source in sources:\n",
    "            print(source)\n",
    "\n",
    "# Attach the button click event to the handler\n",
    "submit_button.on_click(on_submit_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5e524-4279-4566-a56f-9e8a77de00ea",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap: We set up a retrieval-augmented generation (RAG) chatbot using LangChain and Milvus, integrated PDF loading, and improved its functionality.\n",
    "\n",
    "Next Steps: Consider enhancing the chatbot with real-time data retrieval or using more advanced NLP techniques.\n",
    "\n",
    "Resources: For more details, visit [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4) and \n",
    "[Wiki for reference](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
