{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for Research Paper Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c166d0-2deb-41d0-b869-ab3c063d4fcf",
   "metadata": {},
   "source": [
    "### SE research paper chatbot\n",
    "\n",
    "Group Name: [csusb_fall2024_cse6550_team4](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)\n",
    "\n",
    "Instructor: Dr. Alzahrani, Nabeel\n",
    "\n",
    "Course: CSE 6550: Software Engineer Concepts Fall 2024\n",
    "\n",
    "Source: [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082af1f-a595-4647-b840-ea3b214a4e29",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851e3b-26b9-4cdb-8981-22e6b56c1625",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "The purpose of this project is to create an AI-powered research paper chatbot that helps users extract, summarize, and understand content from academic papers. It will offer an interactive Q&A experience, providing accurate, contextually relevant answers to questions about specific sections, making complex research information more accessible and comprehensible.\n",
    "\n",
    "Objective:\n",
    "\n",
    "The Paper Chatbot enhances interaction with academic papers by allowing users to upload documents, ask questions, and receive summaries or clarifications. It simplifies extracting key information, aiding students, researchers, and professionals in efficiently understanding complex content.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Jupyter Notebook, Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32172f-8f08-4c1a-8ed0-8713f13d28ea",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "Checking Python Version\n",
    "\n",
    "- To ensure your system is compatible with the project, follow these steps:\n",
    "\n",
    "- Run Command: Use ```!python --version``` to check the installed Python version.\n",
    "- Verify Version: Confirm that the displayed version is 3.10 or higher.\n",
    "- If the version is lower than 3.10, download and install the latest version from https://www.python.org/downloads/\n",
    "\n",
    "Requirements:\n",
    "- Python must be installed on your system.\n",
    "- The project requires Python 3.10 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80661364-9896-4f81-8bec-ffb9dbbc7228",
   "metadata": {},
   "source": [
    "### Building Virtual Environment\n",
    "- Install Tools: Installs the required Python packages (`ipykernel` and `virtualenv`) without showing any output or warnings during installation.\n",
    "- Create Virtual Environment: Sets up a virtual environment named `chatbot`, a dedicated workspace for your project to manage its dependencies separately from the global system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0ca7d1-7824-46c2-9c91-2a9d6792a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2214545e-003c-47eb-99a5-d203ec424bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!myenv/bin/pip install ipykernel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4816494-f952-41f4-9e1d-89ab2acf35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec myenv in /root/.local/share/jupyter/kernels/myenv\n"
     ]
    }
   ],
   "source": [
    "!myenv/bin/python3 -m ipykernel install --user --name=myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86a898a-7d90-42e0-a7fd-6162b8c6b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/team4_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a84ff-c137-420a-ac48-97631f2e88ac",
   "metadata": {},
   "source": [
    "### Update pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d852f4-6730-4680-84ab-0ccbffba4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/team4_env/lib/python3.11/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18811ba-bf48-4bf7-92ed-8f37f81ab2e6",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "This command installs all the necessary Python packages for your project, including tools for data analysis, machine learning, AI, and visualization. It ensures your environment has everything required to build and run the project seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c92e119-17f2-478e-b661-a06329590a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --root-user-action=ignore -q streamlit pandas jupyter langchain langchain-community langchain-huggingface langchain-text-splitters langchain-mistralai sentence-transformers faiss-cpu transformers matplotlib numpy plotly scikit-learn ipykernel roman yake pymilvus pydantic==2.5.2 mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca9505-f31b-4ecd-8d3a-226f815b63d3",
   "metadata": {},
   "source": [
    "### Initialization and Configuration of Constants\n",
    "\n",
    "Purpose:\n",
    "- Retrieve and Validate the `MISTRAL_API_KEY` Environment Variable\n",
    "\n",
    "Input:\n",
    "1. Environment variables:\n",
    "`MISTRAL_API_KEY` (retrieved using `os.getenv()`).\n",
    "2. Constants:\n",
    "`MILVUS_URI`: Path to the Milvus vector database.\n",
    "`MODEL_NAME`: Name of the embedding model.\n",
    "\n",
    "Output:\n",
    "- Prints confirmation messages:\n",
    "    - Successful import of libraries and setup of constants.\n",
    "    - Values of `MILVUS_URI` and `MODEL_NAME`.\n",
    "- Raises an error if `MISTRAL_API_KEY` is not set:\n",
    "- `ValueError: \"MISTRAL_API_KEY environment variable not set.\"`\n",
    "\n",
    "Processing:\n",
    "- Checks for the `MISTRAL_API_KEY` in the environment:\n",
    "     - If missing, raises a ValueError.\n",
    "- Sets up constants (`MILVUS_URI` and `MODEL_NAME`) for subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c61470-c1c1-4bd6-83f5-65f7da69bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable (Replace 'your-api-key' with your actual API key)\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "# Access the environment variable\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27531a9-789b-4cc1-8657-25884ab84b5c",
   "metadata": {},
   "source": [
    "### Configuring environment variables\n",
    "\n",
    "Purpose: The code initializes an environment for NLP applications, specifically for document retrieval and embedding generation using machine learning models.\n",
    "\n",
    "Input:\n",
    "\n",
    "- Environment variables loaded from a `.env` file (`MISTRAL_API_KEY` and `USER_AGENT`)\n",
    "- Configuration values such as `MILVUS_URI`, `MODEL_NAME`, and `CORPUS_SOURCE`\n",
    "\n",
    "Output:\n",
    "Confirms successful setup of the environment, document source readiness, and embedding model configuration for future processing.\n",
    "\n",
    "Processing:\n",
    "- Environment setup: Loads environment variables to set up API access and a user agent for HTTP requests.\n",
    "- Configuration: Prepares paths and settings for embedding models and vector storage.\n",
    "- Embedding preparation: Sets up a model (`sentence-transformers/all-MiniLM-L6-v2`) for processing documents and generating embeddings.\n",
    "- Document retrieval: References a document source URL (`CORPUS_SOURCE`) as a placeholder for further processing or document loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d114773-049b-41cb-b7fa-84da1b114c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()  # This loads environment variables defined in a .env file into the program\n",
    "\n",
    "# Retrieve the Mistral API key from the environment for authentication\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")  \n",
    "# Set the USER_AGENT variable\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")  \n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT \n",
    "\n",
    "# Configuration settings for Milvus database, model name, and corpus source\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  \n",
    "\n",
    "# Print confirmation of setup\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")  # Confirm successful setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112fc9-ac1b-4d9f-9b3b-c6c915a642e4",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "Purpose: To create and return an embedding function configured with a specified NLP model for generating text embedding\n",
    "\n",
    "Input:\n",
    "- Model Name (`MODEL_NAME`): A pre-defined string representing the name of the model (e.g., `\"sentence-transformers/all-MiniLM-L6-v2\"`). This value is set globally and used to initialize the embedding function.\n",
    "\n",
    "Output:\n",
    "Returns an embedding function configured with the given model. This function can then be used to generate embeddings for text data.\n",
    "\n",
    "Processing:\n",
    "- The function creates an instance of `HuggingFaceEmbeddings` using `MODEL_NAME`, setting up an embedding generator based on the specified model.\n",
    "- It returns this configured embedding generator for use in NLP tasks like document analysis or semantic search.\n",
    "- It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0667306055f54607b7f23488d7f38925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4602108c0ed40c7be06a4d729e5b6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdd6ca65b0049b3a4d2e87fcc540eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886451db4fb04c969aa4d93c9612fc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb4f1473a2e4cfe986547e8036e515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a44f77dadd41b2a0d55fee396ff947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e47ab7faa6c48d2896b370f1d849f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d9f8d0f5ea49f6a31418bc06cbafd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403c4596981841d3b555263284d56703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a86c1181e045c28aa7138f582c327a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce02e50a754350a7b606d3debd3119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function created using model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding function initialized: client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#  Specify the HuggingFace model for embeddings\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns the embedding function for the specified model.\n",
    "\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings: An embedding function object.\n",
    "    \"\"\"\n",
    "    # Initialize the HuggingFace embedding function with the specified model name\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    # Print a message confirming the embedding function has been created\n",
    "    print(f\"Embedding function created using model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Return the initialized embedding function\n",
    "    return embedding_function\n",
    "\n",
    "# Call the embeddin function and display the output\n",
    "embedding_fn = get_embedding_function()  \n",
    "print(f\"Embedding function initialized: {embedding_fn}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Prompt Generator\n",
    "\n",
    "Purpose: To set up configurations for an NLP system and provide a placeholder function for generating prompts.\n",
    "\n",
    "Input:\n",
    "- Model Name: A string (`\"sentence-transformers/all-MiniLM-L6-v2\"`) representing the embedding model.\n",
    "- Milvus URI: A string (`\"./milvus/milvus_vector.db\"`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- A configured environment ready for NLP tasks.\n",
    "- `create_prompt()` function returns a string template to be used for generating detailed research summaries.\n",
    "\n",
    "Processing:\n",
    "- Configuration: Sets the model name for embedding generation and the URI for the vector database connection.\n",
    "- Prompt creation: Defines a simple placeholder function create_prompt() that returns a formatted prompt template for use in text generation.\n",
    "- - It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8cc1b23-745d-4502-98e5-7a6c24a02e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: sentence-transformers/all-MiniLM-L6-v2\n",
      "MILVUS_URI: ./milvus/milvus_vector.db\n",
      "Prompt created: Provide a detailed summary of the latest research on: {input}\n",
      "Returned prompt: Provide a detailed summary of the latest research on: {input}\n"
     ]
    }
   ],
   "source": [
    "# Specifies the HuggingFace model for embedding generation and specifies the URI for the Milvus vector database\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Returns a placeholder prompt template.\n",
    "\n",
    "    Returns:\n",
    "        str: The prompt template.\n",
    "    \"\"\"\n",
    "    # Define a placeholder prompt for generating research summaries\n",
    "    prompt = \"Provide a detailed summary of the latest research on: {input}\"\n",
    "    print(f\"Prompt created: {prompt}\")  # Print confirmation of the created prompt\n",
    "    return prompt  # Return the created prompt template\n",
    "\n",
    "# Display the constants and call the function\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")  # Print the model name for reference\n",
    "print(f\"MILVUS_URI: {MILVUS_URI}\")  # Print the Milvus URI for reference\n",
    "\n",
    "prompt_template = create_prompt()  # Call the function to create a prompt template\n",
    "print(f\"Returned prompt: {prompt_template}\")  # Print the returned prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ee2a-ceea-4610-b143-4ce770555072",
   "metadata": {},
   "source": [
    "### Vector store loader\n",
    "\n",
    "Purpose: To simulate the loading of a vector store from a specified URI and provide an interface for data retrieval.\n",
    "\n",
    "Input:\n",
    "URI: A string representing the location of the vector database (e.g., `MILVUS_URI`).\n",
    "\n",
    "Output:\n",
    "`VectorStore` object with an as_retriever method that simulates returning itself for further operations. This acts as a placeholder for actual vector store functionality.\n",
    "\n",
    "Processing:\n",
    "- The function defines a placeholder `VectorStore` class with an `as_retriever` method that returns the `VectorStore` instance itself, simulating a `vector store` capable of retrieval operations.\n",
    "- The function returns an instance of `VectorStore` initialized when called.\n",
    "- - It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b709a8b-dab0-4ca4-b84b-76931094b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from URI: ./milvus/milvus_vector.db\n",
      "Vector store loaded: <__main__.load_existing_db.<locals>.VectorStore object at 0xffff4c655c50>\n"
     ]
    }
   ],
   "source": [
    "# Function for loading the vector store\n",
    "def load_existing_db(uri):\n",
    "    \"\"\"\n",
    "    Simulates loading an existing vector store from a given URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str): The URI of the vector store database.\n",
    "\n",
    "    Returns:\n",
    "        VectorStore: A mock vector store class instance.\n",
    "    \"\"\"\n",
    "    # Define a mock VectorStore class with an as_retriever method\n",
    "    class VectorStore:\n",
    "        def as_retriever(self):\n",
    "            # Simulate returning a retriever instance\n",
    "            return self\n",
    "\n",
    "    print(f\"Loading vector store from URI: {uri}\")  # Print confirmation of the URI being used\n",
    "    return VectorStore()  # Return an instance of the mock VectorStore class\n",
    "\n",
    "# Specify the URI for the vector store\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # Path to the Milvus vector database\n",
    "\n",
    "# Call the function to load the vector store\n",
    "vector_store = load_existing_db(MILVUS_URI)\n",
    "print(f\"Vector store loaded: {vector_store}\")  # Print confirmation that the vector store was loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a789-ab91-44fb-8473-94745f96c5df",
   "metadata": {},
   "source": [
    "### Document Chain Placeholder\n",
    "\n",
    "Purpose: To serve as a placeholder function for creating a document processing chain using a model and a prompt.\n",
    "\n",
    "Input:\n",
    "- Model: An object representing an NLP model that will be used in the document chain.\n",
    "- Prompt: A string or object representing the prompt template to guide document processing.\n",
    "\n",
    "Output:\n",
    "\n",
    "Returns a string: \"Document chain here.\" to signify that a document processing chain has been set up (used for demonstration or placeholder purposes).\n",
    "\n",
    "Processing:\n",
    "- The function takes the model and prompt as arguments but currently does not perform any operations on them.\n",
    "- It returns a placeholder string indicating that a document chain has been created.\n",
    "- - It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c360e806-8f44-482d-9319-08436048cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document chain with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Using prompt: Provide a detailed summary of the latest research on: {input}\n",
      "Returned document chain: Document chain here.\n"
     ]
    }
   ],
   "source": [
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    \"\"\"\n",
    "    Simulates the creation of a document chain.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model to use.\n",
    "        prompt (str): The prompt template.\n",
    "\n",
    "    Returns:\n",
    "        str: A placeholder message indicating the document chain creation.\n",
    "    \"\"\"\n",
    "    # Print the model being used for creating the document chain\n",
    "    print(f\"Creating document chain with model: {model}\")\n",
    "    \n",
    "    # Print the prompt being used for the document chain\n",
    "    print(f\"Using prompt: {prompt}\")\n",
    "    \n",
    "    # Return a placeholder message for the document chain\n",
    "    return \"Document chain here.\"\n",
    "\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "\n",
    "# Define a test prompt for the document chain\n",
    "test_prompt = \"Provide a detailed summary of the latest research on: {input}\"  \n",
    "\n",
    "# Call the function to simulate creating a document chain\n",
    "document_chain = create_stuff_documents_chain(MODEL_NAME, test_prompt)\n",
    "\n",
    "# Print the result of the document chain creation\n",
    "print(f\"Returned document chain: {document_chain}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dd006-44be-4142-97a3-361340b2a5d0",
   "metadata": {},
   "source": [
    "# 4. Improving the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Improving Chatbot with RAG and Embeddings\n",
    "Purpose: To create and manage a vector store using Milvus, which stores embeddings generated from web documents for NLP applications.\n",
    "\n",
    "Input:\n",
    "- Documents: Text content loaded from the web through the `load_documents_from_web()` function.\n",
    "- Embedding Function: A function that converts document text into numerical embeddings.\n",
    "- URI: A file path (`'./milvus/milvus_vector.db'`) specifying the location of the Milvus vector database.\n",
    "\n",
    "Output:\n",
    "- Prints console messages at each step, confirming the successful execution of loading, processing, and vector store management.\n",
    "- If errors occur, an exception message is printed.\n",
    "\n",
    "Processing:\n",
    "- Document loading: Retrieves documents from a web source.\n",
    "- Document splitting: Splits the loaded documents into manageable chunks.\n",
    "- Embedding generation: Uses an embedding function to generate fixed-size numerical embeddings for each document chunk.\n",
    "- Vector store creation:\n",
    "   - Connects to Milvus.\n",
    "   - Checks if a collection (\"research_paper_chatbot\") exists. If not, creates a new collection schema.\n",
    "   - Inserts document embeddings into the collection.\n",
    "- Vector store loading: Loads an existing vector store collection from Milvus for retrieval operations\n",
    "- It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Creating new Vector Store...\n",
      "New Vector Store Created with provided documents\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'\n",
    "\n",
    "# Placeholder function to simulate loading documents from the web\n",
    "def load_documents_from_web():\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]\n",
    "\n",
    "# Splits documents into smaller chunks\n",
    "def split_documents(documents):\n",
    "    return [doc for doc in documents]\n",
    "\n",
    "# Placeholder function to simulate getting an embedding function\n",
    "def get_embedding_function():\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Collection(name=collection_name)\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)\n",
    "            collection.insert([[embedding]])\n",
    "\n",
    "    return collection\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Loading documents from the web...\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "        print(\"Splitting documents into chunks...\")\n",
    "        docs = split_documents(documents)\n",
    "        print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "        print(\"Getting embedding function...\")\n",
    "        embeddings = get_embedding_function()\n",
    "\n",
    "        uri = './milvus/milvus_vector.db'\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)\n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00ea3b-e59b-43d2-a7d2-1ceebb58dffc",
   "metadata": {},
   "source": [
    "### Query Response Generator\n",
    "Purpose: To provide an entry point for querying a Retrieval-Augmented Generation (RAG) system, generate a response using NLP models, and include source references\n",
    "\n",
    "\n",
    "Input:\n",
    "Query: A string containing the user's question or request for information\n",
    "\n",
    "Output:\n",
    "\n",
    "Response: A string containing the generated answer with source references.\n",
    "Sources: A list of URLs or source identifiers referenced in the response.\n",
    "\n",
    "Processing:\n",
    "1. Model and Prompt Initialization:\n",
    "    - Loads a ChatMistralAI model and creates a prompt template with create_prompt() for structured input.\n",
    "2. Vector Store Retrieval:\n",
    "    - Loads an existing vector store from Milvus and creates a retriever to fetch relevant documents.\n",
    "3. Document Chain Creation:\n",
    "    - Uses `create_stuff_documents_chain()` to set up a document processing chain.\n",
    "4. Retrieval Chain Setup:\n",
    "    - Constructs a retrieval chain that integrates the retriever and document chain to process the query.\n",
    "5. Query Handling and Response Generation:\n",
    "    - Generates a response using the retrieval chain.\n",
    "    - Catches and handles HTTPStatusError for handling service load issues (e.g., error 429 for high traffic).\n",
    "6. Source Attribution:\n",
    "    - Extracts and formats up to four unique sources from the response context to include in the output.\n",
    "    - Appends source references to the generated response.\n",
    "- It takes 2 minutes to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Created Successfully!\n",
      "Prompt Template:\n",
      "Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
      "    Only use the information provided in the <context> tags.\n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    {context}\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    {input}\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    Assistant:\n",
      "Returned PromptTemplate: input_variables=['context', 'input'] template=\"\\n    Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\\n    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\\n    Only use the information provided in the <context> tags.\\n    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n    <context>\\n    {context}\\n    </context>\\n\\n    <question>\\n    {input}\\n    </question>\\n\\n    The response should be specific and use statistics or numbers when possible.\\n\\n    Assistant:\"\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a prompt template for the AI assistant.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The created prompt template.\n",
    "    \"\"\"\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Human: You are an AI assistant, and provides answers to questions by using fact-based and statistical information when possible.\n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "    Only use the information provided in the <context> tags.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {input}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    # Create a PromptTemplate instance\n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    print(\"Prompt Created Successfully!\")\n",
    "    print(f\"Prompt Template:\\n{PROMPT_TEMPLATE.strip()}\")\n",
    "    return prompt\n",
    "\n",
    "# Call the function and print the returned value\n",
    "created_prompt = create_prompt()\n",
    "print(f\"Returned PromptTemplate: {created_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f947a2-7d09-4715-882a-026e2de7f797",
   "metadata": {},
   "source": [
    " ### Adding Evaluation Metrics with Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775a587-3e61-4027-9819-e305f89dbac7",
   "metadata": {},
   "source": [
    "Purpose: To manage and store performance metrics related to a chatbot's classification performance (e.g., True Positives, False Positives, Accuracy, etc.).\n",
    "\n",
    "Inputs:\n",
    "- `increment_value (integer)`: Value to increment or update specific metrics.\n",
    "- `metric (string)`: The name of the metric to update.\n",
    "- `columns (string)`: The columns to fetch from the database.\n",
    "\n",
    "Processing:\n",
    "- Interacts with an database to execute queries.\n",
    "- Performs necessary calculations for metrics like sensitivity, specificity, precision, recall, F1 score. \n",
    "- Safely handles division to avoid division by zero errors.\n",
    "\n",
    "Outputs:\n",
    "Updates or retrieves performance metrics stored in the database.\n",
    "\n",
    "- It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c88ec4d-3664-40a3-a2fc-d3b49bfa25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance_metrics(self):\n",
    "    \"\"\"\n",
    "    Recalculate and update the performance metrics in the database.\n",
    "    \"\"\"\n",
    "    # Fetch metrics from the database\n",
    "    print(\"Fetching performance metrics from the database...\")\n",
    "    metrics = self.get_performance_metrics('true_positive, true_negative, false_positive, false_negative')\n",
    "    print(f\"Metrics retrieved: {metrics}\")  # Print the metrics retrieved for debugging\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = self.safe_division(\n",
    "        metrics['true_positive'] + metrics['true_negative'], \n",
    "        metrics['true_positive'] + metrics['true_negative'] + metrics['false_positive'] + metrics['false_negative']\n",
    "    )\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_positive']\n",
    "    )\n",
    "    \n",
    "    # Calculate sensitivity (also known as recall)\n",
    "    sensitivity = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_negative']\n",
    "    )\n",
    "    \n",
    "    # Calculate specificity\n",
    "    specificity = self.safe_division(\n",
    "        metrics['true_negative'], \n",
    "        metrics['true_negative'] + metrics['false_positive']\n",
    "    )\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = self.safe_division(\n",
    "        metrics['true_positive'], \n",
    "        metrics['true_positive'] + metrics['false_negative']\n",
    "    )\n",
    "\n",
    "    # Print calculated metrics for debugging\n",
    "    print(f\"Calculated Metrics:\\n\"\n",
    "          f\"Accuracy: {accuracy}\\n\"\n",
    "          f\"Precision: {precision}\\n\"\n",
    "          f\"Sensitivity: {sensitivity}\\n\"\n",
    "          f\"Specificity: {specificity}\\n\"\n",
    "          f\"Recall: {recall}\")\n",
    "\n",
    "    # Calculate F1 score if precision and sensitivity are available\n",
    "    if precision and sensitivity:\n",
    "        f1_score = self.safe_division(2 * precision * sensitivity, precision + sensitivity)\n",
    "    else:\n",
    "        f1_score = None  # Handle cases where F1 score cannot be calculated\n",
    "\n",
    "    print(f\"F1 Score: {f1_score}\")  # Print the F1 score for debugging\n",
    "\n",
    "    # Update the metrics in the database\n",
    "    print(\"Updating performance metrics in the database...\")\n",
    "    with self.connection:\n",
    "        self.connection.execute('''\n",
    "            UPDATE performance_metrics\n",
    "            SET accuracy = ?, precision = ?, sensitivity = ?, specificity = ?, f1_score = ?, recall = ?\n",
    "            WHERE id = 1\n",
    "        ''', (accuracy, precision, sensitivity, specificity, f1_score, recall))\n",
    "    print(\"Performance metrics updated successfully.\")  # Confirm the update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff82ec-d178-48b9-8593-5c8098548ca8",
   "metadata": {},
   "source": [
    "### Retrieve an answer from RAG.\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "- The input query is processed using the `preprocess_query` function. This function cleans the query by:\n",
    "- Removing non-alphanumeric characters (e.g., punctuation).\n",
    "- Converting the query to lowercase for better matching during the search.\n",
    "\n",
    "Input:\n",
    "\n",
    "- The user enters a query in the text box `prompt_input`.\n",
    "\n",
    "Output:\n",
    "\n",
    "- The result is displayed in the output widget. This includes:\n",
    "- The generated response.\n",
    "- A list of research papers with titles and links.\n",
    "\n",
    "Processing:\n",
    "\n",
    "- The cleaned query is then passed to the `query_rag` function, which:\n",
    "- Fetches research papers related to the query using the arXiv API.\n",
    "- Simulates generating a response to the query (in a real-world setup, this could use an NLP model like GPT to generate more complex answers).\n",
    "- Extracts paper titles and links from the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d67df23e-967d-4212-946e-865f7ffc4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Create a text input widget for the user to enter a prompt\n",
    "prompt_input = widgets.Text(\n",
    "    value='', \n",
    "    placeholder='Enter your query here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create a button to submit the prompt\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to submit your query'\n",
    ")\n",
    "\n",
    "# Create an output widget to display the response\n",
    "output = widgets.Output()\n",
    "\n",
    "# Helper function to search for relevant papers in arXiv\n",
    "def search_arxiv(query):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.text\n",
    "\n",
    "# Preprocessing function: Clean the query for better search\n",
    "def preprocess_query(query):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    query_cleaned = re.sub(r'[^\\w\\s]', '', query).strip().lower()\n",
    "    return query_cleaned\n",
    "\n",
    "# Processing function to generate answer and retrieve research papers dynamically\n",
    "def query_rag(query):\n",
    "    # Preprocess the input query\n",
    "    query_cleaned = preprocess_query(query)\n",
    "    \n",
    "    # Querying arXiv for relevant research papers\n",
    "    papers_xml = search_arxiv(query_cleaned)\n",
    "    \n",
    "    # Simulating response generation (use a proper NLP model for real use)\n",
    "    response = f\"Based on the query: '{query}', the system suggests the following research papers as references.\"\n",
    "    \n",
    "    # Parse arXiv results to extract paper titles and links\n",
    "    papers = []\n",
    "    # Find all the paper titles and links in the XML response\n",
    "    paper_titles = re.findall(r\"<title>(.*?)</title>\", papers_xml)\n",
    "    paper_links = re.findall(r\"<id>(.*?)</id>\", papers_xml)\n",
    "    \n",
    "    # Skip the first result (query metadata) and pair the titles with their links\n",
    "    for title, link in zip(paper_titles[1:], paper_links[1:]):\n",
    "        papers.append((title.strip(), link.strip()))\n",
    "    \n",
    "    return response, papers\n",
    "\n",
    "# Function to handle button click and display the response\n",
    "def on_submit_click(change):\n",
    "    query = prompt_input.value\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "        \n",
    "        # Process the query and retrieve the response\n",
    "        response, papers = query_rag(query)\n",
    "        \n",
    "        # Display the processed response\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response)\n",
    "        \n",
    "        # Display the suggested research papers\n",
    "        print(\"\\nSuggested Papers:\")\n",
    "        if papers:\n",
    "            for title, link in papers:\n",
    "                print(f\"Title: {title}\\nLink: {link}\\n\")\n",
    "        else:\n",
    "            print(\"No research papers found for the given query.\")\n",
    "\n",
    "# Attach the button click event to the handler\n",
    "submit_button.on_click(on_submit_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f3bc3-1322-4413-b911-001d448dbb45",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot\n",
    "\n",
    "Purpose: Test the chatbot with a sample query\n",
    "\n",
    "Input: Sample queries and documents. \n",
    "\n",
    "Output: Responses and any errors encountered. \n",
    "\n",
    "Processing: Test the RAG model’s response to ensure correctness.\n",
    "\n",
    "- It takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62726eed-9450-4f59-995a-d45b2463972d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a9b0558f884beeb818daa0977ac29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='80%'), placeholder='Enter your query here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dfefe999b842869cc9866c175dcbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle(), tooltip='Click to submit your query'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f53ed128994420abaccdbde140dbf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the widgets\n",
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5e524-4279-4566-a56f-9e8a77de00ea",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap: We set up a retrieval-augmented generation (RAG) chatbot using LangChain and Milvus, integrated PDF loading, and improved its functionality.\n",
    "\n",
    "Next Steps: Consider enhancing the chatbot with real-time data retrieval or using more advanced NLP techniques.\n",
    "\n",
    "Resources: For more details, visit [Github](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4) and \n",
    "[Wiki for reference](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team4/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
