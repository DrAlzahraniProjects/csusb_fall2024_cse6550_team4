{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for Research Paper Chatbot (Team-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851e3b-26b9-4cdb-8981-22e6b56c1625",
   "metadata": {},
   "source": [
    "The Paper Chatbot, is a system designed to enhance user interaction with academic papers by providing a conversational interface. It enables users to upload research documents and ask questions, receive summaries, and clarify complex topics within the text, simplifying the extraction of critical information without reading the entire paper. The system is aimed at students, researchers, and professionals, streamlining their ability to understand and leverage detailed content efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "### Importing libaries\n",
    "This Python script sets up an environment for processing and interacting with documents, particularly academic papers. It imports libraries for tasks like document loading, splitting, embedding generation, and integrates tools such as ChatMistralAI and HuggingFaceEmbeddings, along with a Milvus vector database for efficient data retrieval. Environment variables are loaded for secure API access, and paths are set for data storage and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported libraries successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "#from langchain_cohere import ChatCohere\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from roman import toRoman\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "data_dir = \"./volumes\"\n",
    "print(\"imported libraries successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27531a9-789b-4cc1-8657-25884ab84b5c",
   "metadata": {},
   "source": [
    "### Configuring environment variables\n",
    "\n",
    "Initialize the required environment variables for document processing. `CORPUS_SOURCE` can be updated to change the document source, `MISTRAL_API_KEY` holds the API key for MistralAI, `MILVUS_URI` defines the path to the Milvus Lite database file, and `MODEL_NAME` sets the embedding model used for analyzing the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d114773-049b-41cb-b7fa-84da1b114c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Ensure USER_AGENT environment variable is set; assign a default if missing\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")\n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT\n",
    "\n",
    "# Configuration settings  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # URI for Milvus vector database\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for sentence embeddings\n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  # Document corpus URL\n",
    "\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "\n",
    "- The `get_embedding_function` function then initializes the embedding function with the specified model and prints a confirmation message. \n",
    "\n",
    "- Calling `get_embedding_function` creates and stores this embedding function in `embedding_function`. Finally, it prints a message confirming that the embedding function was created successfully with the specified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    return embedding_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Prompt Generator\n",
    "\n",
    "- The `create_prompt` function serves as a simple template generator for formulating queries related to research topics. When called, it returns a predefined string that instructs the model to provide a detailed summary of the latest research on a specified subject, indicated by the placeholder `{input}`. \n",
    "\n",
    "- This allows the function to be flexible, enabling users to input various research topics and receive contextually relevant summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8cc1b23-745d-4502-98e5-7a6c24a02e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "# Specifies the URI for the Milvus vector database\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"  # Returns a prompt template for querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ee2a-ceea-4610-b143-4ce770555072",
   "metadata": {},
   "source": [
    "### Vector store loader\n",
    "\n",
    "- `load_exisiting_db` function is a placeholder that simulates the loading of a vector store from a specified URI.\n",
    "\n",
    "- It defines a local class, `VectorStore`, which includes a method called `as_retriever` that allows the instance of `VectorStore` to be treated as a retriever. When called, this function returns an instance of the `VectorStore` class, providing a structure for further integration with a retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b709a8b-dab0-4ca4-b84b-76931094b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for loading the vector store\n",
    "def load_exisiting_db(uri):\n",
    "    class VectorStore:  # Defines a local class for the vector store\n",
    "        def as_retriever(self):  # Method to return itself as a retriever\n",
    "            return self  # Returns the instance of VectorStore\n",
    "    return VectorStore()  # Returns an instance of VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a789-ab91-44fb-8473-94745f96c5df",
   "metadata": {},
   "source": [
    "### Document Chain Placeholder\n",
    "\n",
    "- The `create_stuff_documents_chain` function is a placeholder designed to establish a document processing chain that integrates a specified model and a prompt. \n",
    "- Currently, it returns a string that signifies the intended location for the actual document chain implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c360e806-8f44-482d-9319-08436048cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    return \"Document chain here.\"  # Returns a string indicating where the document chain would be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b112bbe-62d0-4e54-b368-ac54a05d3531",
   "metadata": {},
   "source": [
    "### Create Retrieval Chain\n",
    "\n",
    "- This function defines a placeholder for creating a retrieval chain, which integrates a retriever and a document chain to generate a response.\n",
    "  \n",
    "- It returns a lambda function that simulates the retrieval process by providing contextual information and a sample generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0b6231-bb1c-433f-a130-194a7c762568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for creating a retrieval chain\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    return lambda x: {  # Returns a lambda function to generate a response\n",
    "        \"context\": [  # Contextual information with metadata for sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, \n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ], \n",
    "        \"answer\": \"Generated response\"  # Sample generated answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Vector store management (Milvus database)\n",
    "\n",
    "The code sets up a retrieval-augmented generation (RAG) system using a Milvus vector store to manage document embeddings. It includes functions to load documents, create or load a vector store, and manage embedding schemas for storage. Progress is logged with print statements to inform the user of each operation's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Creating new Vector Store...\n",
      "New Vector Store Created with provided documents\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'\n",
    "\n",
    "# Placeholder function to simulate loading documents from the web\n",
    "def load_documents_from_web():\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]\n",
    "\n",
    "# Splits documents into smaller chunks\n",
    "def split_documents(documents):\n",
    "    return [doc for doc in documents]\n",
    "\n",
    "# Placeholder function to simulate getting an embedding function\n",
    "def get_embedding_function():\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Collection(name=collection_name)\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)\n",
    "            collection.insert([[embedding]])\n",
    "\n",
    "    return collection\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"Loading documents from the web...\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "        print(\"Splitting documents into chunks...\")\n",
    "        docs = split_documents(documents)\n",
    "        print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "        print(\"Getting embedding function...\")\n",
    "        embeddings = get_embedding_function()\n",
    "\n",
    "        uri = './milvus/milvus_vector.db'\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)\n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00ea3b-e59b-43d2-a7d2-1ceebb58dffc",
   "metadata": {},
   "source": [
    "### Query Response Generator\n",
    "- The `query_rag` function serves as the main entry point for generating responses using a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "- It initializes the model and sets up the necessary components, including the prompt and retrieval mechanisms, to process a user query. After generating a response, it compiles a list of relevant sources used in the response, ensuring that users receive both the answer and the origins of the information provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a prompt for the model.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: An instance of PromptTemplate for the model.\n",
    "    \"\"\"\n",
    "    # Create a prompt template using the PromptTemplate class\n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"context\"],  # Change to context\n",
    "        template=\"Given the following context: {context}, provide a comprehensive response.\"\n",
    "    )\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    model = ChatMistralAI(model='open-mistral-7b')  # Initializes the ChatMistralAI model\n",
    "    print(\"Model Loaded\")  # Print statement confirming the model has loaded\n",
    "\n",
    "    prompt = create_prompt()  # Calls the function to create the prompt\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)  # Calls the load_existing_db function\n",
    "    retriever = vector_store.as_retriever()  \n",
    "    \n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)  # Creates a document chain\n",
    "        print(\"Document Chain Created\")  # Print confirmation for document chain creation\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)  # Creates a retrieval chain\n",
    "        print(\"Retrieval Chain Created\")  # Print confirmation for retrieval chain creation\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})  # Calls the retrieval chain with the query input\n",
    "    except HTTPStatusError as e:  # Catches HTTP errors\n",
    "        print(f\"HTTPStatusError: {e}\")  # Print the error message\n",
    "        if e.response.status_code == 429:  # Checks for rate limit error\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []  # Returns message for high traffic\n",
    "        return f\"HTTPStatusError: {e}\", []  # Returns error message for other HTTP errors\n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  # Defines maximum number of sources to include\n",
    "    all_sources = \"\"  # Initialize string to hold all source links\n",
    "    sources = []  # Initialize list to hold unique sources\n",
    "    count = 1  # Initialize counter for source numbering\n",
    "\n",
    "    for i in range(max_relevant_sources):  # Loop over the number of maximum relevant sources\n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"]  # Retrieves source from response context\n",
    "            # Check if the source is already added to the list\n",
    "            if source not in sources:  # If source is unique\n",
    "                sources.append(source)  # Add source to the list\n",
    "                all_sources += f\"[Source {count}]({source}), \"  # Append formatted source link to all_sources\n",
    "                count += 1  # Increment the source counter\n",
    "        except IndexError:  # Handle case where there are no more sources\n",
    "            break  # Exit the loop if no more sources are available\n",
    "            \n",
    "    all_sources = all_sources[:-2]  # Remove the last comma and space from all_sources\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"  # Append all sources to the answer\n",
    "    print(\"Response Generated\")  # Print confirmation of response generation\n",
    "\n",
    "    return response[\"answer\"], sources  # Return the generated answer and list of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f3bc3-1322-4413-b911-001d448dbb45",
   "metadata": {},
   "source": [
    "# Retrieve an answer from RAG.\n",
    "\n",
    "Submit a question to the RAG system, get the response, and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfb877-0ed2-4803-8a53-0b4c38ea90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, sources = query_rag(\"How does LLm's work??\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
