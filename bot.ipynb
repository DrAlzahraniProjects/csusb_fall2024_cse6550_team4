{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for bot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153795e0-9f91-43bf-9e5e-cf369190ef1c",
   "metadata": {},
   "source": [
    "### Overview of bot.py\n",
    "\n",
    "- Environment and model setup: Initializes environment variables, loads the Mistral API key, and sets up an embedding model with Hugging Face for document embeddings.\n",
    "\n",
    "- Retrieval chain and prompt creation: Builds a retrieval-augmented chain using Milvus and defines a prompt template to generate concise, source-based responses to queries.\n",
    "\n",
    "- Document processing and vector store management: Loads, splits, and stores documents in a Milvus vector store for efficient retrieval, enabling the chatbot to find and return relevant answers with cited sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae825ba9-ffbd-4730-994b-9e49e2b6ae09",
   "metadata": {},
   "source": [
    "### Import libaries and setup environment variables\n",
    "\n",
    "- `import os` : Import the os module for operating system functionality\n",
    "\n",
    "- `from dotenv import load_dotenv` : Import load_dotenv to load environment variables from a `.env` file\n",
    "\n",
    "- `from langchain.chains.combine_documents import create_stuff_documents_chain` : Import function for combining documents\n",
    "\n",
    "- `from langchain.schema import Document` : Import Document schema from langchain\n",
    "\n",
    "- `from langchain_core.prompts import PromptTemplate` : Import prompt template for generating prompts\n",
    "\n",
    "- `from langchain_mistralai.chat_models import ChatMistralAI` : Import ChatMistralAI model for conversation\n",
    "\n",
    "- `from langchain_milvus import Milvus` : Import Milvus for vector storage\n",
    "\n",
    "- `from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader` : Import loaders for web documents\n",
    "\n",
    "- `from langchain_text_splitters import RecursiveCharacterTextSplitter` : Import splitter for text documents\n",
    "\n",
    "- `from langchain.chains import create_retrieval_chain` : Import function to create a retrieval chain\n",
    "\n",
    "- `from langchain_huggingface import HuggingFaceEmbeddings` : Import HuggingFace embeddings\n",
    "\n",
    "- `from pymilvus import connections, utility` : Import Milvus connection and utility functions\n",
    "\n",
    "- `from requests.exceptions import HTTPError`  : Import HTTPError for handling HTTP exceptions\n",
    "\n",
    "- `from httpx import HTTPStatusError` : Import HTTPStatusError for HTTP status exceptions\n",
    "\n",
    "- This code sets up a chatbot using document retrieval and embeddings. It imports necessary modules, loads environment variables (like MISTRAL_API_KEY), sets a user agent if needed, and configures a Milvus database URI (MILVUS_URI) for storing embeddings.\n",
    "- It also specifies an embedding model (MODEL_NAME) and a document source URL (CORPUS_SOURCE), then confirms the setup with a print statemen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized, documents loaded, embeddings configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Ensure USER_AGENT environment variable is set; assign a default if missing\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"my_custom_user_agent\")\n",
    "os.environ[\"USER_AGENT\"] = USER_AGENT\n",
    "\n",
    "# Configuration settings  \n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # URI for Milvus vector database\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for sentence embeddings\n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  # Document corpus URL\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Environment initialized, documents loaded, embeddings configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "\n",
    "This code sets up an embedding function using the Hugging Face model sentence-transformers/all-MiniLM-L6-v2. First, it disables tqdm warnings by setting an environment variable. The `get_embedding_function` function then initializes the embedding function with the specified model and prints a confirmation message. Calling `get_embedding_function` creates and stores this embedding function in `embedding_function`. Finally, it prints a message confirming that the embedding function was created successfully with the specified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns Hugging Face model embedding function.\n",
      "Embedding function created with model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Suppress tqdm warnings by setting environment variable\n",
    "os.environ['TQDM_DISABLE'] = '1'  \n",
    "# Defines the model name for embeddings\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "\n",
    "# Creates embedding function using specified model\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model.\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)  \n",
    "    \n",
    "    print(\"Returns Hugging Face model embedding function.\")  \n",
    "    \n",
    "    return embedding_function  # Returns the created embedding function\n",
    "\n",
    "# Calls the function and stores the resulting embedding function \n",
    "embedding_function = get_embedding_function()  \n",
    "\n",
    "# Prints confirmation of the embedding function creation along with the model name\n",
    "print(f\"Embedding function created with model: {MODEL_NAME}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Query response generation\n",
    "\n",
    "- This code implements a retrieval-augmented generation (RAG) chatbot that answers questions using a document retrieval model. It initializes the ChatMistralAI language model, defines a prompt template, and sets up a Milvus vector store for embeddings.\n",
    "  \n",
    "- The query_rag() function processes a query, loads the necessary components, and generates a response that includes relevant source links. An example query demonstrates how the chatbot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Document Chain Created\n",
      "Retrieval Chain Created\n",
      "Response Generated\n",
      "Initializes components, retrieves documents, generates response.\n",
      "Generated response\n",
      "\n",
      "Sources: [Source 1](https://example.com/research_paper1), [Source 2](https://example.com/research_paper2)\n"
     ]
    }
   ],
   "source": [
    "# Defines the model name for embeddings\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "# Specifies the URI for the Milvus vector database\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  \n",
    "\n",
    "# Placeholder function for creating a prompt\n",
    "def create_prompt():\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"  # Returns a prompt template for querying\n",
    "\n",
    "# Placeholder function for loading the vector store\n",
    "def load_exisiting_db(uri):\n",
    "    class VectorStore:  # Defines a local class for the vector store\n",
    "        def as_retriever(self):  # Method to return itself as a retriever\n",
    "            return self  # Returns the instance of VectorStore\n",
    "    return VectorStore()  # Returns an instance of VectorStore\n",
    "\n",
    "# Placeholder function for creating a document chain\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    return \"Document chain here.\"  # Returns a string indicating where the document chain would be\n",
    "\n",
    "# Placeholder function for creating a retrieval chain\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    return lambda x: {  # Returns a lambda function to generate a response\n",
    "        \"context\": [  # Contextual information with metadata for sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, \n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ], \n",
    "        \"answer\": \"Generated response\"  # Sample generated answer\n",
    "    }\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query.\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    model = ChatMistralAI(model='open-mistral-7b')  # Initializes the ChatMistralAI model\n",
    "    print(\"Model Loaded\")  # Print statement confirming the model has loaded\n",
    "\n",
    "    prompt = create_prompt()  # Calls the function to create the prompt\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_exisiting_db(uri=MILVUS_URI)  \n",
    "    retriever = vector_store.as_retriever()  \n",
    "    \n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)  # Creates a document chain\n",
    "        print(\"Document Chain Created\")  # Print confirmation for document chain creation\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)  # Creates a retrieval chain\n",
    "        print(\"Retrieval Chain Created\")  # Print confirmation for retrieval chain creation\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})  # Calls the retrieval chain with the query input\n",
    "    except HTTPStatusError as e:  # Catches HTTP errors\n",
    "        print(f\"HTTPStatusError: {e}\")  # Print the error message\n",
    "        if e.response.status_code == 429:  # Checks for rate limit error\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []  # Returns message for high traffic\n",
    "        return f\"HTTPStatusError: {e}\", []  # Returns error message for other HTTP errors\n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  # Defines maximum number of sources to include\n",
    "    all_sources = \"\"  # Initialize string to hold all source links\n",
    "    sources = []  # Initialize list to hold unique sources\n",
    "    count = 1  # Initialize counter for source numbering\n",
    "\n",
    "    for i in range(max_relevant_sources):  # Loop over the number of maximum relevant sources\n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"]  # Retrieves source from response context\n",
    "            # Check if the source is already added to the list\n",
    "            if source not in sources:  # If source is unique\n",
    "                sources.append(source)  # Add source to the list\n",
    "                all_sources += f\"[Source {count}]({source}), \"  # Append formatted source link to all_sources\n",
    "                count += 1  # Increment the source counter\n",
    "        except IndexError:  # Handle case where there are no more sources\n",
    "            break  # Exit the loop if no more sources are available\n",
    "            \n",
    "    all_sources = all_sources[:-2]  # Remove the last comma and space from all_sources\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"  # Append all sources to the answer\n",
    "    print(\"Response Generated\")  # Print confirmation of response generation\n",
    "\n",
    "    print(\"Initializes components, retrieves documents, generates response.\")  # Summary of what the function does\n",
    "\n",
    "    return response[\"answer\"], sources  # Return the generated answer and list of sources\n",
    "\n",
    "# Example usage\n",
    "query = \"Latest research on machine learning in healthcare\"  # Defines an example query\n",
    "answer, sources = query_rag(query)  # Calls the query_rag function with the example query\n",
    "print(answer)  # Prints the generated answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Vector store management\n",
    "\n",
    "The code sets up a retrieval-augmented generation (RAG) system using a Milvus vector store to manage document embeddings. It includes functions to load documents, create or load a vector store, and manage embedding schemas for storage. Progress is logged with print statements to inform the user of each operation's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0549bd-63e5-4f1e-84aa-eb9fd7e82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 3 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 3 chunks.\n",
      "Getting embedding function...\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Collection already exists. Loading existing Vector Store.\n",
      "Existing Vector Store Loaded\n",
      "Vector store created successfully.\n",
      "Loading existing vector store...\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Loaded existing vector store successfully.\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Placeholder function to simulate loading documents from the web.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of loaded document contents.\n",
    "    \"\"\"\n",
    "    return [\"Document 1 content\", \"Document 2 content\", \"Document 3 content\"]  # Example document contents\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of documents to split.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of split document chunks.\n",
    "    \"\"\"\n",
    "    return [doc for doc in documents]  # Example: returns the documents as-is\n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Placeholder function to simulate getting an embedding function.\n",
    "    \n",
    "    Returns:\n",
    "        function: A simulated embedding function.\n",
    "    \"\"\"\n",
    "    def embedding_function(doc):\n",
    "        return [0.1] * 512  # Example fixed-size embedding\n",
    "    return embedding_function\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "    It connects to a local Milvus database specified by the URI. If a collection named \"research_paper_chatbot\" already exists,\n",
    "    it loads the existing vector store; otherwise, it creates a new vector store and drops any existing one.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings: A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)  # Split the URI into directory and file name\n",
    "    os.makedirs(head[0], exist_ok=True)  # Create directory if it doesn't exist\n",
    "    print(\"Directory created for vector store if it did not exist\")  # Log directory creation\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri) \n",
    "    print(\"Connected to the Milvus database\")  \n",
    "\n",
    "    # Define collection name\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(collection_name):  \n",
    "        print(\"Collection already exists. Loading existing Vector Store.\") \n",
    "        vector_store = Collection(name=collection_name)  # Load existing vector store\n",
    "        print(\"Existing Vector Store Loaded\")  # Log successful load of existing store\n",
    "    else:\n",
    "        print(\"Creating new Vector Store...\")\n",
    "        fields = [\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),  # Adjust dimensions as necessary\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields=fields, description=\"Collection for research paper embeddings\")\n",
    "        collection = Collection(name=collection_name, schema=schema)  # Create new collection\n",
    "        print(\"New Vector Store Created with provided documents\")  # Log new store creation\n",
    "\n",
    "        # Insert documents into the vector store\n",
    "        for doc in docs:\n",
    "            embedding = embeddings(doc)  # Generate embedding\n",
    "            collection.insert([[embedding]])  # Insert the embedding into the collection\n",
    "\n",
    "    return vector_store  # Return the created or loaded vector store\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str): Path to the local milvus db.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created.\n",
    "    \"\"\"\n",
    "    collection_name = \"research_paper_chatbot\"\n",
    "    vector_store = Collection(name=collection_name)  # Load existing vector store\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")  # Log successful load of existing store\n",
    "    return vector_store  # Return the loaded vector store\n",
    "\n",
    "if __name__ == '__main__':  # Check if this script is being executed directly\n",
    "    try:\n",
    "        # Load documents from the web\n",
    "        print(\"Loading documents from the web...\")  \n",
    "        documents = load_documents_from_web()  \n",
    "        print(f\"Loaded {len(documents)} documents from the web.\")  # Log number of loaded documents\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        print(\"Splitting documents into chunks...\")  # Log document splitting start\n",
    "        docs = split_documents(documents)  # Split documents into smaller chunks\n",
    "        print(f\"Split into {len(docs)} chunks.\")  # Log number of chunks created\n",
    "\n",
    "        # Get the embedding function\n",
    "        print(\"Getting embedding function...\")  # Log embedding function retrieval\n",
    "        embeddings = get_embedding_function()  # Retrieve the embedding function\n",
    "\n",
    "        # Define the URI for the Milvus database\n",
    "        uri = './milvus/milvus_vector.db'  # Set the URI variable for the database\n",
    "\n",
    "        # Create the vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)  \n",
    "        print(\"Vector store created successfully.\")\n",
    "\n",
    "        # Load the existing vector store\n",
    "        print(\"Loading existing vector store...\")\n",
    "        loaded_vector_store = load_exisiting_db(uri)  \n",
    "        print(\"Loaded existing vector store successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")  # Print any error that occurs\n",
    "\n",
    "    print(\"Finished operations.\")  # Log completion of operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
