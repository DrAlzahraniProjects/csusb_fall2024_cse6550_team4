{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d050be-da98-4d8b-84cc-64aa0a12153c",
   "metadata": {},
   "source": [
    "# Documentation for statistics_chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe0b06-fb55-4d4c-a634-83b9d1b5030a",
   "metadata": {},
   "source": [
    "#### Overview of statistics \n",
    "\n",
    "This code performs: \n",
    "\n",
    "- Initializes a statistics dictionary to keep track of metrics related to user interaction, such as the number of questions asked, correct and incorrect answers, user engagement, response times, accuracy rate, and a confusion matrix for performance analysis\n",
    "\n",
    "- Updates statistics dynamically based on user interactions, including updating the count of questions and answers, total response time, and daily performance metrics, while also recalculating the accuracy rate\n",
    "\n",
    "- Calculates performance metrics using functions from sklearn, like sensitivity, specificity, accuracy, precision, recall, and F1 score, and updates the confusion matrix accordingly for a more detailed analysis\n",
    "\n",
    "- Resets and reinitializes statistics for fresh tracking, while optionally preserving the total number of questions asked, allowing for periodic or session-based data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7cd12-9433-4bfd-87f4-a856133db359",
   "metadata": {},
   "source": [
    "### Statistics tracking\n",
    "\n",
    "#### Importing a library and initializing the statistics dictionary\n",
    "\n",
    "- `datetime` module: Imports the datetime module to handle date and time operations for tracking interactions\n",
    "\n",
    "- `\"number_of_questions\": 0` - Tracks the total number of questions asked by users\n",
    "\n",
    "- `\"number_of_correct_answers\": 0` - Counts the total number of correct answers given by the bot\n",
    "\n",
    "- `\"number_of_incorrect_answers\": 0,` - Counts the total number of incorrect answers given by the bot\n",
    "\n",
    "- `\"user_engagement_metrics\": 0` - Placeholder for user engagement metrics, can be refined later for details\n",
    "\n",
    "- `\"total_response_time\": 0,` - Cumulative response time for all interactions, measured in seconds\n",
    "\n",
    "- `\"accuracy_rate\": 0.0` - Accuracy rate of the bot, calculated as a percentage\n",
    "\n",
    "- `\"user_satisfaction\": [],` - List to store user satisfaction ratings for each interaction\n",
    "\n",
    "- `\"feedback_summary\": [],`- List to store feedback provided by users for future improvements\n",
    "\n",
    "- `\"daily_statistics\": {}`  - Dictionary to hold daily statistics, enabling daily analysis of interaction\n",
    "\n",
    "- `\"confusion_matrix\": None` - Placeholder for storing the results of classification metrics, which can be computed later\n",
    "\n",
    "This code initializes a statistics dictionary to track various metrics related to user interactions and performance. It includes counts of questions asked, correct and incorrect answers, user engagement metrics, and total response time, while also calculating accuracy and gathering user satisfaction and feedback. Additionally, it allows for summarizing daily statistics for ongoing performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22e5ffd6-7b8d-4677-8a7b-20b5e2750edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks user interactions, performance, and satisfaction metrics.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime  \n",
    "\n",
    "# Initialize statistics dictionary to hold all relevant metrics\n",
    "statistics = {\n",
    "    \"number_of_questions\": 0,  \n",
    "    \"number_of_correct_answers\": 0,  \n",
    "    \"number_of_incorrect_answers\": 0,  \n",
    "    \"user_engagement_metrics\": 0,  \n",
    "    \"total_response_time\": 0,  \n",
    "    \"accuracy_rate\": 0.0,  \n",
    "    \"user_satisfaction\": [], \n",
    "    \"feedback_summary\": [],  \n",
    "    \"daily_statistics\": {},\n",
    "    \"confusion_matrix\": None  # Initialize confusion_matrix to None\n",
    "}\n",
    "# Print the statistics dictionary\n",
    "print(\"Tracks user interactions, performance, and satisfaction metrics.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eff8ac-a835-499a-a23d-428fb7f1b893",
   "metadata": {},
   "source": [
    "### Statistics update function\n",
    "\n",
    "- This code defines the update_statistics() function to track and display chatbot performance metrics, including total questions asked, correct and incorrect answers, total and average response times, and accuracy rates.\n",
    "  \n",
    "-  It updates daily statistics and user feedback. The function is demonstrated through two example calls, showing how statistics are updated after each user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57d2f9f8-1f81-4948-838c-b1dc99e8bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Statistics:\n",
      "Total Questions Asked: 1\n",
      "Correct Answers: 1\n",
      "Incorrect Answers: 0\n",
      "Total Response Time (s): 1.2\n",
      "Average Response Time (s): 1.2\n",
      "Accuracy Rate (%): 100.0\n",
      "Daily Statistics: {'2024-11-04': {'questions_asked': 1, 'correct_answers': 1}}\n",
      "User Satisfaction Ratings: []\n",
      "Feedback Summary: []\n",
      "\n",
      "Updated Statistics:\n",
      "Total Questions Asked: 2\n",
      "Correct Answers: 2\n",
      "Incorrect Answers: 0\n",
      "Total Response Time (s): 2.7\n",
      "Average Response Time (s): 1.35\n",
      "Accuracy Rate (%): 100.0\n",
      "Daily Statistics: {'2024-11-04': {'questions_asked': 2, 'correct_answers': 2}}\n",
      "User Satisfaction Ratings: []\n",
      "Feedback Summary: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def update_statistics(user_input, bot_response, response_time, correct_answer=True, is_new_question=True):\n",
    "    # Only increment number of questions if it's a new question\n",
    "    if is_new_question:\n",
    "        statistics[\"number_of_questions\"] += 1\n",
    "    \n",
    "    # Update total response time for average calculations\n",
    "    statistics[\"total_response_time\"] += response_time\n",
    "\n",
    "    # Update correctness\n",
    "    if correct_answer:\n",
    "        statistics[\"number_of_correct_answers\"] += 1\n",
    "    else:\n",
    "        statistics[\"number_of_incorrect_answers\"] += 1\n",
    "\n",
    "    # Update daily stats\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    if today not in statistics[\"daily_statistics\"]:\n",
    "        statistics[\"daily_statistics\"][today] = {\"questions_asked\": 0, \"correct_answers\": 0}\n",
    "    statistics[\"daily_statistics\"][today][\"questions_asked\"] += 1 if is_new_question else 0\n",
    "    if correct_answer:\n",
    "        statistics[\"daily_statistics\"][today][\"correct_answers\"] += 1\n",
    "\n",
    "    # Calculate accuracy rate\n",
    "    if statistics[\"number_of_questions\"] > 0:\n",
    "        statistics[\"accuracy_rate\"] = (statistics[\"number_of_correct_answers\"] / statistics[\"number_of_questions\"]) * 100\n",
    "\n",
    "    # Print updated statistics in a structured format\n",
    "    print(\"Updated Statistics:\")\n",
    "    print(f\"Total Questions Asked: {statistics['number_of_questions']}\")\n",
    "    print(f\"Correct Answers: {statistics['number_of_correct_answers']}\")\n",
    "    print(f\"Incorrect Answers: {statistics['number_of_incorrect_answers']}\")\n",
    "    print(f\"Total Response Time (s): {statistics['total_response_time']}\")\n",
    "    print(f\"Average Response Time (s): {round(statistics['total_response_time'] / max(1, statistics['number_of_questions']), 2)}\")\n",
    "    print(f\"Accuracy Rate (%): {round(statistics['accuracy_rate'], 2)}\")\n",
    "    print(f\"Daily Statistics: {statistics['daily_statistics']}\")\n",
    "    print(f\"User Satisfaction Ratings: {statistics['user_satisfaction']}\")\n",
    "    print(f\"Feedback Summary: {statistics['feedback_summary']}\\n\")\n",
    "\n",
    "\n",
    "# Example of a generic call to update_statistics\n",
    "update_statistics(\n",
    "    user_input=\"How does a convolutional neural network work?\",\n",
    "    bot_response=\"A convolutional neural network uses convolutional layers to detect patterns in input data.\",\n",
    "    response_time=1.2,  # Example response time in seconds\n",
    "    correct_answer=True,\n",
    "    is_new_question=True\n",
    ")\n",
    "\n",
    "# Call the function again to demonstrate updated output\n",
    "update_statistics(\n",
    "    user_input=\"What are the benefits of using AI in healthcare?\",\n",
    "    bot_response=\"AI can improve diagnosis accuracy and streamline administrative tasks.\",\n",
    "    response_time=1.5,\n",
    "    correct_answer=True,\n",
    "    is_new_question=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f04b73-85e6-4215-9915-ac40f6d41e99",
   "metadata": {},
   "source": [
    "### Statistics display function\n",
    "\n",
    "- This code defines the `get_statistics_display()` function, which prints various chatbot interaction statistics, including the number of questions, correct and incorrect answers, user engagement metrics, average response time, accuracy rate, and user satisfaction.\n",
    "\n",
    "- It updates statistics through two example interactions with the chatbot and then calls the function to display the updated statistics in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa62a1e1-3497-4fa2-b75f-360726146617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Statistics:\n",
      "Total Questions Asked: 3\n",
      "Correct Answers: 3\n",
      "Incorrect Answers: 0\n",
      "Total Response Time (s): 3.9000000000000004\n",
      "Average Response Time (s): 1.3\n",
      "Accuracy Rate (%): 100.0\n",
      "Daily Statistics: {'2024-11-04': {'questions_asked': 3, 'correct_answers': 3}}\n",
      "User Satisfaction Ratings: []\n",
      "Feedback Summary: []\n",
      "\n",
      "Updated Statistics:\n",
      "Total Questions Asked: 4\n",
      "Correct Answers: 4\n",
      "Incorrect Answers: 0\n",
      "Total Response Time (s): 5.4\n",
      "Average Response Time (s): 1.35\n",
      "Accuracy Rate (%): 100.0\n",
      "Daily Statistics: {'2024-11-04': {'questions_asked': 4, 'correct_answers': 4}}\n",
      "User Satisfaction Ratings: []\n",
      "Feedback Summary: []\n",
      "\n",
      "Statistics Display:\n",
      "Number of Questions: 4\n",
      "Number of Correct Answers: 4\n",
      "Number of Incorrect Answers: 0\n",
      "User Engagement Metrics: 0\n",
      "Avg Response Time (s): 1.35\n",
      "Accuracy Rate (%): 100.0\n",
      "User Satisfaction Ratings: []\n",
      "Feedback Summary: []\n",
      "Daily Statistics: {'2024-11-04': {'questions_asked': 4, 'correct_answers': 4}}\n",
      "Confusion Matrix: Not computed yet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_statistics_display():\n",
    "    # Prepare statistics for display\n",
    "    stats_display = {\n",
    "        \"Number of Questions\": statistics[\"number_of_questions\"],\n",
    "        \"Number of Correct Answers\": statistics[\"number_of_correct_answers\"],\n",
    "        \"Number of Incorrect Answers\": statistics[\"number_of_incorrect_answers\"],\n",
    "        \"User Engagement Metrics\": statistics[\"user_engagement_metrics\"],\n",
    "        \"Avg Response Time (s)\": round(statistics[\"total_response_time\"] / max(1, statistics[\"number_of_questions\"]), 2),\n",
    "        \"Accuracy Rate (%)\": round(statistics[\"accuracy_rate\"], 2),\n",
    "        \"User Satisfaction Ratings\": statistics[\"user_satisfaction\"],\n",
    "        \"Feedback Summary\": statistics[\"feedback_summary\"],\n",
    "        \"Daily Statistics\": statistics[\"daily_statistics\"],\n",
    "        \"Confusion Matrix\": statistics[\"confusion_matrix\"] if statistics[\"confusion_matrix\"] is not None else \"Not computed yet\"\n",
    "    }\n",
    "    \n",
    "    # Print the statistics in a structured format\n",
    "    print(\"Statistics Display:\")\n",
    "    for key, value in stats_display.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()  # Blank line for better readability\n",
    "\n",
    "# Example of updating statistics with user interactions\n",
    "update_statistics(\n",
    "    user_input=\"How does a convolutional neural network work?\",\n",
    "    bot_response=\"A convolutional neural network uses convolutional layers to detect patterns in input data.\",\n",
    "    response_time=1.2,  # Example response time in seconds\n",
    "    correct_answer=True,\n",
    "    is_new_question=True\n",
    ")\n",
    "\n",
    "update_statistics(\n",
    "    user_input=\"What are the benefits of using AI in healthcare?\",\n",
    "    bot_response=\"AI can improve diagnosis accuracy and streamline administrative tasks.\",\n",
    "    response_time=1.5,\n",
    "    correct_answer=True,\n",
    "    is_new_question=True\n",
    ")\n",
    "\n",
    "# Call the function to display statistics\n",
    "get_statistics_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bbe8c-ee4e-45d1-9cae-537ca8d6615b",
   "metadata": {},
   "source": [
    "### Evaluates model performance:\n",
    "\n",
    "- The `compute_metrics` function evaluates a classification model by calculating key performance metrics: confusion matrix, sensitivity, specificity, accuracy, precision, recall, and F1 score, using true labels `(y_true)` and predicted labels `(y_pred)`. \n",
    "\n",
    "- It stores the confusion matrix in the `statistics` dictionary and prints both the matrix and the calculated metrics. \n",
    "\n",
    "- Example demonstrates its usage with sample true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75310d2d-2157-4e74-b0da-ad794af20ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [1 4]]\n",
      "\n",
      "Metrics:\n",
      "Sensitivity: 0.8\n",
      "Specificity: 0.8\n",
      "Accuracy: 0.8\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4, 1],\n",
       "        [1, 4]]),\n",
       " {'Sensitivity': 0.8,\n",
       "  'Specificity': 0.8,\n",
       "  'Accuracy': 0.8,\n",
       "  'Precision': 0.8,\n",
       "  'Recall': 0.8,\n",
       "  'F1 Score': 0.8})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# fuction to Calculate classification performance metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Update statistics with the computed confusion matrix\n",
    "    statistics[\"confusion_matrix\"] = cm\n",
    "\n",
    "    # Prepare metrics for display\n",
    "    metrics = {\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "    \n",
    "    # Print the confusion matrix and metrics\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nMetrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    return cm, metrics\n",
    "\n",
    "# Example of using compute_metrics\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]  # True labels\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]  # Predicted labels\n",
    "\n",
    "compute_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855201af-b91d-475b-82c3-8e312533f451",
   "metadata": {},
   "source": [
    "### Resets performance metrics\n",
    "\n",
    "- The `reset_statistics` function resets the `statistics` dictionary while retaining the count of total questions asked. It prints the current statistics before clearing most metrics, setting values like correct answers, incorrect answers, user engagement, and feedback to zero or empty. \n",
    "\n",
    "- After the reset, it confirms the changes by displaying the updated statistics, enabling ongoing performance tracking while allowing for periodic data clearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "021d2c96-d785-4d06-af4b-323f6a63c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Statistics Before Reset:\n",
      "number_of_questions: 4\n",
      "number_of_correct_answers: 4\n",
      "number_of_incorrect_answers: 0\n",
      "user_engagement_metrics: 0\n",
      "total_response_time: 5.4\n",
      "accuracy_rate: 100.0\n",
      "user_satisfaction: []\n",
      "feedback_summary: []\n",
      "daily_statistics: {'2024-11-04': {'questions_asked': 4, 'correct_answers': 4}}\n",
      "confusion_matrix: None\n",
      "\n",
      "Statistics have been reset.\n",
      "number_of_questions: 4\n",
      "number_of_correct_answers: 0\n",
      "number_of_incorrect_answers: 0\n",
      "user_engagement_metrics: 0\n",
      "total_response_time: 0\n",
      "accuracy_rate: 0.0\n",
      "user_satisfaction: []\n",
      "feedback_summary: []\n",
      "daily_statistics: {}\n",
      "confusion_matrix: None\n"
     ]
    }
   ],
   "source": [
    "def reset_statistics():\n",
    "    global statistics\n",
    "\n",
    "    # Print current statistics before resetting\n",
    "    print(\"Current Statistics Before Reset:\")\n",
    "    for key, value in statistics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Reset statistics\n",
    "    statistics = {\n",
    "        \"number_of_questions\": statistics[\"number_of_questions\"],  # Keep this value\n",
    "        \"number_of_correct_answers\": 0,\n",
    "        \"number_of_incorrect_answers\": 0,\n",
    "        \"user_engagement_metrics\": 0,\n",
    "        \"total_response_time\": 0,\n",
    "        \"accuracy_rate\": 0.0,\n",
    "        \"user_satisfaction\": [],\n",
    "        \"feedback_summary\": [],\n",
    "        \"daily_statistics\": {},\n",
    "        \"confusion_matrix\": None\n",
    "    }\n",
    "\n",
    "    # Print the reset statistics\n",
    "    print(\"\\nStatistics have been reset.\")\n",
    "    for key, value in statistics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Example call to reset statistics\n",
    "reset_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
